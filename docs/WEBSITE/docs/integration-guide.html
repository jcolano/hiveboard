<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Integration Guide — HiveBoard Docs</title>
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600;700&family=Plus+Jakarta+Sans:wght@400;500;600;700;800&display=swap" rel="stylesheet">
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
<style>
/* ═══════════════════════════════════════
   DESIGN TOKENS (HiveBoard system)
   ═══════════════════════════════════════ */
:root {
    --accent: #c2410c;
    --accent-dim: rgba(194, 65, 12, 0.06);
    --accent-hover: #a93b0b;
    --accent-light: rgba(194, 65, 12, 0.1);
    --font-mono: 'IBM Plex Mono', monospace;
    --font-sans: 'Plus Jakarta Sans', sans-serif;
    --radius-sm: 6px;
    --radius-md: 10px;
    --bg-deep: #f5f3ef;
    --bg-primary: #ffffff;
    --bg-card: #ffffff;
    --bg-elevated: #fafaf8;
    --bg-hover: #f0eeea;
    --border: #e2e0db;
    --border-subtle: #eceae5;
    --text-primary: #1a1a1a;
    --text-secondary: #4a4a4a;
    --text-muted: #8a8a8a;
    --success: #16a34a;
    --success-dim: rgba(22, 163, 74, 0.08);
    --error: #dc2626;
    --error-dim: rgba(220, 38, 38, 0.06);
    --warning: #d97706;
    --warning-dim: rgba(217, 119, 6, 0.08);
    --info: #2563eb;
    --info-dim: rgba(37, 99, 235, 0.06);
}

* { margin: 0; padding: 0; box-sizing: border-box; }

body {
    font-family: var(--font-sans);
    background: var(--bg-deep);
    color: var(--text-primary);
    -webkit-font-smoothing: antialiased;
    min-height: 100vh;
}

@keyframes fade-in { from { opacity: 0; } to { opacity: 1; } }
@keyframes fade-in-up { from { opacity: 0; transform: translateY(12px); } to { opacity: 1; transform: translateY(0); } }

/* ═══════════════════════════════════════
   TOP BAR
   ═══════════════════════════════════════ */
.topbar {
    display: flex; align-items: center; justify-content: space-between;
    padding: 0 24px; height: 56px;
    background: var(--bg-primary); border-bottom: 1px solid var(--border);
    position: fixed; top: 0; left: 0; right: 0; z-index: 100;
}
.topbar-left { display: flex; align-items: center; gap: 20px; }
.logo {
    display: flex; align-items: center; gap: 10px;
    font-family: var(--font-sans); font-weight: 800; font-size: 17px;
    letter-spacing: -0.5px; text-decoration: none; color: var(--text-primary);
}
.logo-hex {
    width: 28px; height: 28px; background: var(--accent);
    clip-path: polygon(50% 0%, 100% 25%, 100% 75%, 50% 100%, 0% 75%, 0% 25%);
    display: flex; align-items: center; justify-content: center; flex-shrink: 0;
}
.logo-hex-inner {
    width: 18px; height: 18px; background: var(--bg-primary);
    clip-path: polygon(50% 0%, 100% 25%, 100% 75%, 50% 100%, 0% 75%, 0% 25%);
}
.logo span { color: var(--accent); }

.docs-badge {
    font-family: var(--font-mono); font-size: 11px; font-weight: 600;
    color: var(--accent); background: var(--accent-dim);
    padding: 3px 10px; border-radius: 4px;
    text-transform: uppercase; letter-spacing: 0.5px;
    border: 1px solid rgba(194, 65, 12, 0.12);
}

.topbar-right { display: flex; align-items: center; gap: 12px; }
.topbar-link {
    font-family: var(--font-sans); font-size: 13px; font-weight: 600;
    color: var(--text-secondary); text-decoration: none;
    padding: 7px 14px; border-radius: var(--radius-sm);
    transition: all 0.15s; border: 1px solid transparent;
}
.topbar-link:hover { background: var(--bg-hover); color: var(--text-primary); }
.topbar-link.primary {
    color: #fff; background: var(--accent); border-color: var(--accent);
}
.topbar-link.primary:hover { background: var(--accent-hover); }

/* ═══════════════════════════════════════
   LAYOUT
   ═══════════════════════════════════════ */
.docs-layout {
    display: flex; min-height: 100vh; padding-top: 56px;
}

/* ─── LEFT SIDEBAR ─── */
.docs-sidebar {
    width: 260px; flex-shrink: 0;
    background: var(--bg-primary);
    border-right: 1px solid var(--border);
    padding: 20px 12px;
    position: fixed; top: 56px; bottom: 0; left: 0;
    overflow-y: auto;
    scrollbar-width: thin;
    scrollbar-color: var(--border) transparent;
}
.docs-sidebar::-webkit-scrollbar { width: 4px; }
.docs-sidebar::-webkit-scrollbar-thumb { background: var(--border); border-radius: 2px; }

.docs-nav-section {
    font-family: var(--font-sans); font-size: 10px; font-weight: 700;
    text-transform: uppercase; letter-spacing: 0.8px;
    color: var(--text-muted); padding: 16px 12px 6px;
}
.docs-nav-section:first-child { padding-top: 0; }

.docs-nav-item {
    display: flex; align-items: center; gap: 10px;
    padding: 9px 12px; border-radius: var(--radius-sm);
    font-family: var(--font-sans); font-size: 13.5px; font-weight: 500;
    color: var(--text-secondary); text-decoration: none;
    transition: all 0.12s; border: 1px solid transparent;
}
.docs-nav-item:hover {
    background: var(--bg-hover); color: var(--text-primary);
}
.docs-nav-item.active {
    background: var(--accent-dim); color: var(--accent);
    font-weight: 600; border-color: rgba(194, 65, 12, 0.1);
}
.docs-nav-icon {
    width: 18px; height: 18px; flex-shrink: 0; opacity: 0.5;
    display: flex; align-items: center;
}
.docs-nav-icon svg { width: 18px; height: 18px; }
.docs-nav-item.active .docs-nav-icon { opacity: 1; color: var(--accent); }

/* ─── MAIN CONTENT ─── */
.docs-main {
    flex: 1; margin-left: 260px; margin-right: 220px;
    padding: 40px 48px 80px;
    max-width: 820px;
    animation: fade-in 0.3s ease;
}

/* ─── RIGHT TOC ─── */
.page-toc {
    position: fixed; top: 96px; right: 24px;
    width: 196px; max-height: calc(100vh - 120px);
    overflow-y: auto; scrollbar-width: thin;
    scrollbar-color: var(--border) transparent;
}
.page-toc::-webkit-scrollbar { width: 3px; }
.page-toc::-webkit-scrollbar-thumb { background: var(--border); border-radius: 2px; }

.page-toc-title {
    font-family: var(--font-sans); font-size: 10px; font-weight: 700;
    text-transform: uppercase; letter-spacing: 0.8px;
    color: var(--text-muted); padding-bottom: 8px;
    border-bottom: 1px solid var(--border-subtle);
    margin-bottom: 8px;
}
.toc-link {
    display: block; padding: 4px 0;
    font-family: var(--font-sans); font-size: 12.5px; font-weight: 500;
    color: var(--text-muted); text-decoration: none;
    transition: color 0.15s;
    line-height: 1.4;
    border-left: 2px solid transparent;
    padding-left: 10px;
    margin-left: -1px;
}
.toc-link:hover { color: var(--text-primary); }
.toc-link.toc-h3 { padding-left: 22px; font-size: 12px; }
.toc-link.active {
    color: var(--accent); font-weight: 600;
    border-left-color: var(--accent);
}

/* ═══════════════════════════════════════
   CONTENT TYPOGRAPHY
   ═══════════════════════════════════════ */
.doc-content h1 {
    font-family: var(--font-sans); font-size: 28px; font-weight: 800;
    letter-spacing: -0.7px; line-height: 1.2;
    margin-bottom: 8px; color: var(--text-primary);
}
.doc-content h2 {
    font-family: var(--font-sans); font-size: 21px; font-weight: 700;
    letter-spacing: -0.3px; line-height: 1.3;
    margin-top: 48px; margin-bottom: 16px;
    padding-top: 24px; border-top: 1px solid var(--border-subtle);
    color: var(--text-primary);
}
.doc-content h2:first-of-type { margin-top: 32px; border-top: none; padding-top: 0; }

.doc-content h3 {
    font-family: var(--font-sans); font-size: 17px; font-weight: 700;
    margin-top: 32px; margin-bottom: 12px;
    color: var(--text-primary);
}
.doc-content h4 {
    font-family: var(--font-sans); font-size: 15px; font-weight: 700;
    margin-top: 24px; margin-bottom: 8px;
    color: var(--text-secondary);
}

.heading-anchor {
    color: var(--accent); text-decoration: none;
    opacity: 0; font-weight: 400; margin-left: 6px;
    transition: opacity 0.15s;
}
h2:hover .heading-anchor,
h3:hover .heading-anchor,
h4:hover .heading-anchor { opacity: 0.5; }
.heading-anchor:hover { opacity: 1 !important; }

.doc-content p {
    font-size: 15px; line-height: 1.7;
    color: var(--text-secondary); margin-bottom: 16px;
}
.doc-content strong { color: var(--text-primary); font-weight: 600; }

.doc-content a {
    color: var(--accent); text-decoration: none;
    border-bottom: 1px solid rgba(194, 65, 12, 0.2);
    transition: border-color 0.15s;
}
.doc-content a:hover { border-bottom-color: var(--accent); }

.doc-content ul, .doc-content ol {
    margin-bottom: 16px; padding-left: 24px;
}
.doc-content li {
    font-size: 15px; line-height: 1.7;
    color: var(--text-secondary); margin-bottom: 6px;
}
.doc-content li strong { color: var(--text-primary); }

/* Inline code */
.doc-content code {
    font-family: var(--font-mono); font-size: 13px; font-weight: 500;
    color: var(--accent); background: var(--accent-dim);
    padding: 2px 6px; border-radius: 4px;
    border: 1px solid rgba(194, 65, 12, 0.08);
}

/* Code blocks */
.code-block {
    position: relative; margin-bottom: 20px;
    background: #1e1e2e; border-radius: var(--radius-md);
    border: 1px solid #2a2a3e;
    overflow: hidden;
}
.code-block .code-lang {
    position: absolute; top: 0; right: 0;
    font-family: var(--font-mono); font-size: 10px; font-weight: 600;
    text-transform: uppercase; letter-spacing: 0.5px;
    color: #8888aa; background: rgba(255,255,255,0.05);
    padding: 4px 12px; border-radius: 0 var(--radius-md) 0 6px;
}
.code-block pre {
    margin: 0; padding: 20px 24px; overflow-x: auto;
    scrollbar-width: thin; scrollbar-color: #444 transparent;
}
.code-block pre::-webkit-scrollbar { height: 4px; }
.code-block pre::-webkit-scrollbar-thumb { background: #555; border-radius: 2px; }
.code-block pre code {
    font-family: var(--font-mono); font-size: 13px; line-height: 1.6;
    color: #cdd6f4; background: none; padding: 0; border: none; border-radius: 0;
}

/* Tables */
.table-wrapper {
    margin-bottom: 20px; overflow-x: auto;
    border: 1px solid var(--border); border-radius: var(--radius-md);
    scrollbar-width: thin;
}
.table-wrapper table {
    width: 100%; border-collapse: collapse;
    font-size: 14px;
}
.table-wrapper th {
    font-family: var(--font-sans); font-size: 12px; font-weight: 700;
    text-transform: uppercase; letter-spacing: 0.4px;
    color: var(--text-muted); background: var(--bg-elevated);
    text-align: left; padding: 10px 16px;
    border-bottom: 1px solid var(--border);
}
.table-wrapper td {
    font-family: var(--font-sans); font-size: 14px; line-height: 1.5;
    color: var(--text-secondary); padding: 10px 16px;
    border-bottom: 1px solid var(--border-subtle);
    vertical-align: top;
}
.table-wrapper tr:last-child td { border-bottom: none; }
.table-wrapper tr:hover td { background: var(--bg-elevated); }
.table-wrapper td code {
    font-size: 12px;
}

/* Blockquotes / Callouts */
.callout {
    margin-bottom: 20px; padding: 16px 20px;
    border-left: 3px solid var(--border);
    border-radius: 0 var(--radius-sm) var(--radius-sm) 0;
    background: var(--bg-elevated);
}
.callout p { margin-bottom: 8px; font-size: 14px; }
.callout p:last-child { margin-bottom: 0; }
.callout em { color: var(--text-muted); }

.callout-warning {
    border-left-color: var(--warning);
    background: var(--warning-dim);
}
.callout-tip {
    border-left-color: var(--success);
    background: var(--success-dim);
}
.callout-info {
    border-left-color: var(--info);
    background: var(--info-dim);
}
.callout-danger {
    border-left-color: var(--error);
    background: var(--error-dim);
}

/* HR */
.doc-content hr {
    border: none; height: 1px;
    background: var(--border); margin: 32px 0;
}

/* ─── VERSION BADGE ─── */
.doc-meta {
    display: flex; align-items: center; gap: 12px;
    margin-bottom: 24px;
}
.doc-meta-badge {
    font-family: var(--font-mono); font-size: 11px; font-weight: 600;
    color: var(--text-muted); background: var(--bg-elevated);
    padding: 3px 10px; border-radius: 4px;
    border: 1px solid var(--border-subtle);
}

/* ─── PREV / NEXT NAV ─── */
.prev-next {
    display: flex; gap: 16px; margin-top: 48px;
    padding-top: 24px; border-top: 1px solid var(--border);
}
.prev-next-link {
    flex: 1; display: block; padding: 16px 20px;
    background: var(--bg-primary); border: 1px solid var(--border);
    border-radius: var(--radius-md); text-decoration: none;
    transition: all 0.15s;
}
.prev-next-link:hover {
    border-color: var(--accent); background: var(--accent-dim);
    transform: translateY(-1px);
    box-shadow: 0 4px 12px rgba(194, 65, 12, 0.06);
}
.prev-next-link.next { text-align: right; }
.prev-next-label {
    font-family: var(--font-sans); font-size: 12px; font-weight: 600;
    color: var(--text-muted); display: block; margin-bottom: 4px;
}
.prev-next-title {
    font-family: var(--font-sans); font-size: 15px; font-weight: 700;
    color: var(--text-primary);
}
.prev-next-link:hover .prev-next-title { color: var(--accent); }

/* ─── MOBILE MENU ─── */
.mobile-menu-toggle {
    display: none; align-items: center; justify-content: center;
    width: 36px; height: 36px; border: 1px solid var(--border);
    border-radius: var(--radius-sm); background: var(--bg-primary);
    cursor: pointer; color: var(--text-secondary);
}

/* ═══════════════════════════════════════
   RESPONSIVE
   ═══════════════════════════════════════ */
@media (max-width: 1200px) {
    .page-toc { display: none; }
    .docs-main { margin-right: 0; }
}

@media (max-width: 860px) {
    .mobile-menu-toggle { display: flex; }
    .docs-sidebar {
        transform: translateX(-100%);
        transition: transform 0.25s ease;
        z-index: 50;
        box-shadow: none;
    }
    .docs-sidebar.open {
        transform: translateX(0);
        box-shadow: 8px 0 30px rgba(0,0,0,0.1);
    }
    .docs-main {
        margin-left: 0; padding: 28px 20px 60px;
    }
    .doc-content h1 { font-size: 24px; }
    .doc-content h2 { font-size: 19px; }
    .prev-next { flex-direction: column; }
}

/* ═══════════════════════════════════════
   PRISM THEME OVERRIDES (dark code blocks)
   ═══════════════════════════════════════ */
.code-block .token.comment,
.code-block .token.prolog,
.code-block .token.doctype { color: #6c7086; }
.code-block .token.punctuation { color: #a6adc8; }
.code-block .token.property,
.code-block .token.tag,
.code-block .token.boolean,
.code-block .token.number { color: #fab387; }
.code-block .token.string,
.code-block .token.attr-value { color: #a6e3a1; }
.code-block .token.selector,
.code-block .token.attr-name,
.code-block .token.builtin { color: #89b4fa; }
.code-block .token.keyword { color: #cba6f7; }
.code-block .token.function { color: #89b4fa; }
.code-block .token.operator { color: #89dceb; }
.code-block .token.class-name { color: #f9e2af; }
.code-block .token.decorator { color: #f38ba8; }
</style>
</head>
<body>

<!-- TOP BAR -->
<div class="topbar">
    <div class="topbar-left">
        <a href="home.html" class="logo">
            <div class="logo-hex"><div class="logo-hex-inner"></div></div>
            Hive<span>Board</span>
        </a>
        <span class="docs-badge">Docs</span>
    </div>
    <div class="topbar-right">
        <button class="mobile-menu-toggle" id="mobileMenuToggle" aria-label="Toggle navigation">
            <svg width="18" height="18" viewBox="0 0 18 18" fill="none"><path d="M3 5h12M3 9h12M3 13h12" stroke="currentColor" stroke-width="1.4" stroke-linecap="round"/></svg>
        </button>
        <a href="https://github.com/hiveboard/hiveloop" class="topbar-link" target="_blank" rel="noopener">GitHub</a>
        <a href="home.html" class="topbar-link primary">Open Dashboard</a>
    </div>
</div>

<!-- LAYOUT -->
<div class="docs-layout">
    <!-- SIDEBAR -->
    <nav class="docs-sidebar" id="docsSidebar">
<div class="docs-nav-section">Getting Started</div>
<a class="docs-nav-item" href="user-manual.html"><span class="docs-nav-icon"><svg viewBox="0 0 18 18" fill="none"><path d="M2 3.5A1.5 1.5 0 013.5 2H7a2 2 0 012 2v12.5l-.5-.5-3-3H3.5A1.5 1.5 0 012 11.5v-8z" stroke="currentColor" stroke-width="1.3"/><path d="M16 3.5A1.5 1.5 0 0014.5 2H11a2 2 0 00-2 2v12.5l.5-.5 3-3h2A1.5 1.5 0 0016 11.5v-8z" stroke="currentColor" stroke-width="1.3"/></svg></span><span>SDK Manual</span></a>
<a class="docs-nav-item active" href="integration-guide.html"><span class="docs-nav-icon"><svg viewBox="0 0 18 18" fill="none"><path d="M6 2v4M12 2v4M4 6h10v3a5 5 0 01-5 5 5 5 0 01-5-5V6zM9 14v3" stroke="currentColor" stroke-width="1.3" stroke-linecap="round" stroke-linejoin="round"/></svg></span><span>Integration Guide</span></a>
<div class="docs-nav-section">Dashboard</div>
<a class="docs-nav-item" href="docs-dashboard-guide.html"><span class="docs-nav-icon"><svg viewBox="0 0 18 18" fill="none"><rect x="2" y="2" width="14" height="14" rx="2" stroke="currentColor" stroke-width="1.3"/><path d="M2 7h14M7 7v9" stroke="currentColor" stroke-width="1.3"/></svg></span><span>Dashboard Guide</span></a>
<div class="docs-nav-section">Instrumentation</div>
<a class="docs-nav-item" href="docs-instrumentation-guide.html"><span class="docs-nav-icon"><svg viewBox="0 0 18 18" fill="none"><circle cx="9" cy="9" r="7" stroke="currentColor" stroke-width="1.3"/><circle cx="9" cy="9" r="4" stroke="currentColor" stroke-width="1.3"/><circle cx="9" cy="9" r="1" fill="currentColor"/></svg></span><span>Instrumentation</span></a>
<a class="docs-nav-item" href="docs-layer1-guide.html"><span class="docs-nav-icon"><svg viewBox="0 0 18 18" fill="none"><path d="M9 2L2 6l7 4 7-4-7-4zM2 12l7 4 7-4M2 9l7 4 7-4" stroke="currentColor" stroke-width="1.3" stroke-linecap="round" stroke-linejoin="round"/></svg></span><span>Layer 1 Guide</span></a>
<div class="docs-nav-section">Rich Events</div>
<a class="docs-nav-item" href="docs-layer2-rich-events.html"><span class="docs-nav-icon"><svg viewBox="0 0 18 18" fill="none"><path d="M10 2L3 10h5l-1 6 7-8h-5l1-6z" stroke="currentColor" stroke-width="1.3" stroke-linecap="round" stroke-linejoin="round"/></svg></span><span>Rich Events</span></a>
<a class="docs-nav-item" href="docs-layer2-llm-tracking.html"><span class="docs-nav-icon"><svg viewBox="0 0 18 18" fill="none"><rect x="4" y="4" width="10" height="10" rx="1.5" stroke="currentColor" stroke-width="1.3"/><rect x="7" y="7" width="4" height="4" rx="0.5" stroke="currentColor" stroke-width="1.3"/><path d="M7 2v2M11 2v2M7 14v2M11 14v2M2 7h2M2 11h2M14 7h2M14 11h2" stroke="currentColor" stroke-width="1.3" stroke-linecap="round"/></svg></span><span>LLM Tracking</span></a>
<a class="docs-nav-item" href="docs-operational-events.html"><span class="docs-nav-icon"><svg viewBox="0 0 18 18" fill="none"><path d="M16 9h-3l-2 7L7 2 5 9H2" stroke="currentColor" stroke-width="1.3" stroke-linecap="round" stroke-linejoin="round"/></svg></span><span>Operational Events</span></a>
<a class="docs-nav-item" href="docs-track-context.html"><span class="docs-nav-icon"><svg viewBox="0 0 18 18" fill="none"><circle cx="5" cy="5" r="2" stroke="currentColor" stroke-width="1.3"/><circle cx="13" cy="5" r="2" stroke="currentColor" stroke-width="1.3"/><circle cx="5" cy="13" r="2" stroke="currentColor" stroke-width="1.3"/><path d="M5 7v4M13 7c0 3-2 4-8 4" stroke="currentColor" stroke-width="1.3"/></svg></span><span>Track Context</span></a>

    </nav>

    <!-- MAIN CONTENT -->
    <main class="docs-main">
        <div class="doc-meta">
            <span class="doc-meta-badge">v0.1.0</span>
            <span class="doc-meta-badge">Updated Feb 2026</span>
        </div>
        <article class="doc-content">
<h1 id="hiveboard-integration-guide-for-agentic-frameworks">HiveBoard Integration Guide for Agentic Frameworks <a class="heading-anchor" href="#hiveboard-integration-guide-for-agentic-frameworks" aria-label="Link to this section">#</a></h1>
<p><strong>A layer-by-layer technical guide to making your AI agents observable.</strong></p>
<p>This guide is for developers who build or maintain agentic AI frameworks -- systems where an LLM reasons, selects tools, executes them, and repeats until a task is complete. The guide assumes Python but the concepts apply to any language with an HiveLoop SDK.</p>
<p>The integration follows a layered approach. Each layer builds on the previous one, and each one unlocks new dashboard capabilities. You can ship after any layer and come back later.</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Layer</th>
  <th>What you add</th>
  <th>What the dashboard shows</th>
  <th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
  <td>0</td>
  <td>SDK init + agent registration</td>
  <td>Heartbeat, online/offline, stuck detection</td>
  <td>~10 lines</td>
</tr>
<tr>
  <td>1</td>
  <td>Task boundaries + action tracking</td>
  <td>Task table, timelines, success/failure rates</td>
  <td>~30 lines</td>
</tr>
<tr>
  <td>2a</td>
  <td>LLM call tracking</td>
  <td>Cost explorer, token usage, model breakdown</td>
  <td>~10 lines per LLM call site</td>
</tr>
<tr>
  <td>2b</td>
  <td>Tool execution tracking</td>
  <td>Tool nodes in timeline with inputs/outputs</td>
  <td>~15 lines at tool dispatch</td>
</tr>
<tr>
  <td>2c</td>
  <td>Rich events</td>
  <td>Plans, escalations, approvals, issues, TODOs, queue state</td>
  <td>~5 lines per event site</td>
</tr>
<tr>
  <td>3</td>
  <td>Advanced observability events</td>
  <td>Insights tab: learning, compaction, config, memory ops</td>
  <td>~10 lines per event site</td>
</tr>
<tr>
  <td>4</td>
  <td>Client-side detectors</td>
  <td>Insights tab: cycle detection, prompt bloat, state drift</td>
  <td>~50 lines per detector</td>
</tr>
</tbody>
</table></div>
<hr />
<h2 id="prerequisites">Prerequisites <a class="heading-anchor" href="#prerequisites" aria-label="Link to this section">#</a></h2>
<div class="code-block"><span class="code-lang">bash</span><pre><code class="language-bash">pip install hiveloop
</code></pre></div>
<p>You need:</p>
<ul>
<li>A HiveBoard server (self-hosted or cloud) with an API key (<code>hb_live_...</code> or <code>hb_test_...</code>)</li>
<li>A project created on the server (a slug like <code>&quot;my-project&quot;</code>)</li>
</ul>
<hr />
<h2 id="layer-0-initialization-and-agent-registration">Layer 0: Initialization and Agent Registration <a class="heading-anchor" href="#layer-0-initialization-and-agent-registration" aria-label="Link to this section">#</a></h2>
<p><strong>Goal:</strong> Get agents visible on the dashboard with heartbeats and stuck detection.</p>
<h3 id="step-1-initialize-the-sdk">Step 1: Initialize the SDK <a class="heading-anchor" href="#step-1-initialize-the-sdk" aria-label="Link to this section">#</a></h3>
<p>Call <code>hiveloop.init()</code> once at application startup -- before any agents are created.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">import hiveloop

hb = hiveloop.init(
    api_key=&quot;hb_live_your_key_here&quot;,
    endpoint=&quot;https://your-hiveboard-server.com&quot;,
    environment=&quot;production&quot;,
)
</code></pre></div>
<p>This creates a singleton client that buffers events in memory and flushes them to the server every 5 seconds via a background thread. It never blocks your application code. If the server is unreachable, events are queued (up to 10,000) and retried with exponential backoff.</p>
<p><strong>Where to put this:</strong> In your application's entry point -- the <code>main()</code> function, the FastAPI <code>lifespan</code>, the CLI command handler, or wherever your framework boots up.</p>
<p><strong>Testing:</strong> <code>hiveloop.init()</code> is a singleton -- subsequent calls log a warning and return the existing instance. In test suites where you need a fresh SDK state between tests, call <code>hiveloop.reset()</code> in your teardown to flush pending events, stop background threads, and clear the singleton.</p>
<p><code>hiveloop.init()</code> parameters:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Parameter</th>
  <th>Type</th>
  <th>Default</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>api_key</code></td>
  <td>str</td>
  <td>required</td>
  <td>Starts with <code>hb_live_</code>, <code>hb_test_</code>, or <code>hb_read_</code></td>
</tr>
<tr>
  <td><code>endpoint</code></td>
  <td>str</td>
  <td><code>&quot;https://api.hiveboard.io&quot;</code></td>
  <td>Your HiveBoard server URL</td>
</tr>
<tr>
  <td><code>environment</code></td>
  <td>str</td>
  <td><code>&quot;production&quot;</code></td>
  <td>Filterable on dashboard. Use <code>&quot;development&quot;</code>, <code>&quot;staging&quot;</code>, etc.</td>
</tr>
<tr>
  <td><code>flush_interval</code></td>
  <td>float</td>
  <td><code>5.0</code></td>
  <td>Seconds between automatic flushes</td>
</tr>
<tr>
  <td><code>batch_size</code></td>
  <td>int</td>
  <td><code>100</code></td>
  <td>Max events per HTTP request (server caps at 500)</td>
</tr>
<tr>
  <td><code>max_queue_size</code></td>
  <td>int</td>
  <td><code>10000</code></td>
  <td>Events buffered in memory before oldest are dropped</td>
</tr>
<tr>
  <td><code>debug</code></td>
  <td>bool</td>
  <td><code>False</code></td>
  <td>Logs SDK operations to stderr</td>
</tr>
</tbody>
</table></div>
<h3 id="step-2-register-agents">Step 2: Register agents <a class="heading-anchor" href="#step-2-register-agents" aria-label="Link to this section">#</a></h3>
<p>For each agent in your framework, call <code>hb.agent()</code>. This is idempotent -- calling it twice with the same <code>agent_id</code> returns the same handle.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">hiveloop_agent = hb.agent(
    agent_id=&quot;sales&quot;,
    type=&quot;sales&quot;,                    # role or classification
    version=&quot;claude-sonnet-4-5&quot;,     # model or agent version
    framework=&quot;my-framework&quot;,        # your framework name
    heartbeat_interval=30,           # seconds between heartbeats (0 to disable)
    stuck_threshold=300,             # seconds without heartbeat before &quot;stuck&quot; badge
)
</code></pre></div>
<p><strong>Where to put this:</strong> Wherever your framework creates or initializes agent instances. If agents are created lazily, call <code>hb.agent()</code> right after the agent object is constructed.</p>
<p><strong>Store the handle.</strong> You'll need it later for task tracking, tool tracking, and event reporting. Common patterns:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># Option A: Store on the agent object
agent._hiveloop = hb.agent(agent_id=agent.id, ...)

# Option B: Store in a registry dict
_hiveloop_agents[agent.id] = hb.agent(agent_id=agent.id, ...)
</code></pre></div>
<p><code>hb.agent()</code> parameters:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Parameter</th>
  <th>Type</th>
  <th>Default</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>agent_id</code></td>
  <td>str</td>
  <td>required</td>
  <td>Unique identifier, max 256 chars</td>
</tr>
<tr>
  <td><code>type</code></td>
  <td>str</td>
  <td><code>&quot;general&quot;</code></td>
  <td>Agent classification (role, department, etc.)</td>
</tr>
<tr>
  <td><code>version</code></td>
  <td>str</td>
  <td><code>None</code></td>
  <td>Agent or model version string</td>
</tr>
<tr>
  <td><code>framework</code></td>
  <td>str</td>
  <td><code>&quot;custom&quot;</code></td>
  <td>Your framework name</td>
</tr>
<tr>
  <td><code>heartbeat_interval</code></td>
  <td>int</td>
  <td><code>30</code></td>
  <td>Seconds. Set to <code>0</code> to disable heartbeats</td>
</tr>
<tr>
  <td><code>stuck_threshold</code></td>
  <td>int</td>
  <td><code>300</code></td>
  <td>Seconds. Agent shows STUCK badge if no heartbeat within this window</td>
</tr>
<tr>
  <td><code>heartbeat_payload</code></td>
  <td>Callable</td>
  <td><code>None</code></td>
  <td><code>() -&gt; dict</code> called each heartbeat, included in payload</td>
</tr>
<tr>
  <td><code>queue_provider</code></td>
  <td>Callable</td>
  <td><code>None</code></td>
  <td><code>() -&gt; dict</code> returning queue state each heartbeat</td>
</tr>
</tbody>
</table></div>
<h3 id="heartbeat-payload-rich-agent-telemetry">Heartbeat payload: Rich agent telemetry <a class="heading-anchor" href="#heartbeat-payload-rich-agent-telemetry" aria-label="Link to this section">#</a></h3>
<p>The <code>heartbeat_payload</code> callback is called every heartbeat cycle (default 30s). Use it to attach runtime metrics that flow into the dashboard's agent cards and the Insights tab. The callback must return a <code>dict</code> — keep it small (under 2KB) and fast (under 10ms).</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">def make_heartbeat_payload(agent_obj):
    &quot;&quot;&quot;Build a heartbeat payload callback for an agent.&quot;&quot;&quot;
    _boot_time = time.time()

    def payload():
        return {
            # Uptime and lifecycle
            &quot;uptime_seconds&quot;: int(time.time() - _boot_time),
            &quot;version&quot;: agent_obj.config.version,

            # Cumulative token/cost counters (since boot)
            &quot;total_tokens_in&quot;: agent_obj.metrics.total_tokens_in,
            &quot;total_tokens_out&quot;: agent_obj.metrics.total_tokens_out,
            &quot;total_cost_usd&quot;: round(agent_obj.metrics.total_cost, 4),

            # Error counters
            &quot;total_errors&quot;: agent_obj.metrics.error_count,
            &quot;consecutive_failures&quot;: agent_obj.metrics.consecutive_failures,
            &quot;last_error&quot;: agent_obj.metrics.last_error_message,

            # Current state
            &quot;current_model&quot;: agent_obj.current_model,
            &quot;tasks_completed&quot;: agent_obj.metrics.tasks_completed,
            &quot;tasks_failed&quot;: agent_obj.metrics.tasks_failed,
        }
    return payload

# At agent registration
hiveloop_agent = hb.agent(
    agent_id=&quot;sales&quot;,
    type=&quot;sales&quot;,
    version=&quot;claude-sonnet-4-5&quot;,
    framework=&quot;my-framework&quot;,
    heartbeat_payload=make_heartbeat_payload(agent_obj),
    queue_provider=make_queue_provider(&quot;sales&quot;),
)
</code></pre></div>
<p><strong>What this unlocks:</strong> The Insights tab uses heartbeat payload fields for fleet-wide health views — total cost across all agents, error rate trends, uptime monitoring, and context pressure metrics. Without a rich heartbeat payload, these views show empty or incomplete data.</p>
<p><strong>Keep it lean.</strong> The callback runs every 30 seconds per agent. Don't call external APIs, read files, or do anything that could block or fail. Read from in-memory counters only. If a field isn't available yet, omit it rather than returning <code>None</code>.</p>
<h3 id="what-you-see-on-the-dashboard-after-layer-0">What you see on the dashboard after Layer 0 <a class="heading-anchor" href="#what-you-see-on-the-dashboard-after-layer-0" aria-label="Link to this section">#</a></h3>
<ul>
<li><strong>Agent cards</strong> appear in The Hive panel with IDLE status</li>
<li><strong>Heartbeat sparklines</strong> pulse every 30 seconds</li>
<li><strong>STUCK detection</strong> fires if an agent stops responding</li>
<li><strong>Connection indicator</strong> shows green &quot;Connected&quot;</li>
</ul>
<h3 id="safety-contract">Safety contract <a class="heading-anchor" href="#safety-contract" aria-label="Link to this section">#</a></h3>
<p>Every HiveLoop call should be wrapped so it never crashes your agent:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># Guard pattern -- use everywhere
if hiveloop_agent:
    try:
        hiveloop_agent.some_method(...)
    except Exception:
        pass  # observability must never break the agent
</code></pre></div>
<p>This is non-negotiable. Observability is a side channel. If the HiveBoard server goes down, your agents must continue running identically.</p>
<hr />
<h2 id="layer-1-task-boundaries-and-action-tracking">Layer 1: Task Boundaries and Action Tracking <a class="heading-anchor" href="#layer-1-task-boundaries-and-action-tracking" aria-label="Link to this section">#</a></h2>
<p><strong>Goal:</strong> Get task timelines, success/failure tracking, and duration measurement.</p>
<h3 id="concept-what-is-a-quottaskquot">Concept: What is a &quot;task&quot;? <a class="heading-anchor" href="#concept-what-is-a-quottaskquot" aria-label="Link to this section">#</a></h3>
<p>A task is a single unit of work your agent performs from start to finish. In most frameworks this maps to:</p>
<ul>
<li>One execution of the agentic loop (receive message -&gt; reason -&gt; use tools -&gt; respond)</li>
<li>One event being processed from a queue</li>
<li>One API request being handled</li>
<li>One scheduled job running</li>
</ul>
<p>The key question: <strong>&quot;Where does a unit of work start and end?&quot;</strong> Find that boundary in your code.</p>
<h3 id="step-3-wrap-task-execution-with-agenttask">Step 3: Wrap task execution with <code>agent.task()</code> <a class="heading-anchor" href="#step-3-wrap-task-execution-with-agenttask" aria-label="Link to this section">#</a></h3>
<p><code>agent.task()</code> is a context manager. It emits <code>task_started</code> on entry and <code>task_completed</code> or <code>task_failed</code> on exit (depending on whether an exception was raised).</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">def execute_agent(agent, message, event_context=None):
    hiveloop_agent = agent._hiveloop  # the handle from Layer 0

    if hiveloop_agent is not None:
        task_id = generate_unique_task_id()  # must be unique per execution
        with hiveloop_agent.task(
            task_id,
            project=&quot;my-project&quot;,
            type=event_context.get(&quot;source&quot;, &quot;api&quot;) if event_context else &quot;api&quot;,
        ) as task:
            # Make the task handle available deeper in the call stack
            set_current_task(task)
            set_hiveloop_agent(hiveloop_agent)
            try:
                result = agent.run(message)
            finally:
                clear_current_task()
                clear_hiveloop_agent()
        return result
    else:
        return agent.run(message)
</code></pre></div>
<p><strong>Critical: Task IDs must be unique per execution.</strong> If you reuse task IDs (like using a session ID), the dashboard will show only one row per ID instead of one row per run. Generate unique IDs using the event ID, a UUID, or a combination:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">def generate_unique_task_id(agent_id, event_id=None):
    if event_id:
        return f&quot;{agent_id}-{event_id}&quot;
    return f&quot;{agent_id}-{uuid.uuid4().hex[:8]}&quot;
</code></pre></div>
<p><code>agent.task()</code> parameters:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Parameter</th>
  <th>Type</th>
  <th>Default</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>task_id</code></td>
  <td>str</td>
  <td>required</td>
  <td>Unique per execution</td>
</tr>
<tr>
  <td><code>project</code></td>
  <td>str</td>
  <td><code>None</code></td>
  <td>Groups tasks in the dashboard</td>
</tr>
<tr>
  <td><code>type</code></td>
  <td>str</td>
  <td><code>None</code></td>
  <td>Classification: <code>&quot;heartbeat&quot;</code>, <code>&quot;webhook&quot;</code>, <code>&quot;human&quot;</code>, <code>&quot;api&quot;</code>, etc.</td>
</tr>
</tbody>
</table></div>
<h3 id="alternative-manual-task-lifecycle-with-agentstart_task">Alternative: Manual task lifecycle with <code>agent.start_task()</code> <a class="heading-anchor" href="#alternative-manual-task-lifecycle-with-agentstart_task" aria-label="Link to this section">#</a></h3>
<p>If your framework doesn't support context managers (e.g., callback-driven or event-sourced architectures where start and end happen in different functions), use <code>start_task()</code> with explicit <code>complete()</code>/<code>fail()</code> calls:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">task = hiveloop_agent.start_task(
    task_id,
    project=&quot;my-project&quot;,
    type=&quot;webhook&quot;,
)
set_current_task(task)

# ... later, when the work finishes:
try:
    result = do_work()
    task.complete()              # emits task_completed
except Exception as exc:
    task.fail(exception=exc)     # emits task_failed
finally:
    clear_current_task()
</code></pre></div>
<p><code>agent.start_task()</code> accepts the same parameters as <code>agent.task()</code>. The caller is responsible for calling <code>task.complete()</code> or <code>task.fail()</code> -- if neither is called, the task will appear stuck on the dashboard.</p>
<h3 id="step-4-plumb-the-task-handle-through-the-call-stack">Step 4: Plumb the task handle through the call stack <a class="heading-anchor" href="#step-4-plumb-the-task-handle-through-the-call-stack" aria-label="Link to this section">#</a></h3>
<p>The task handle created by <code>agent.task()</code> needs to be accessible from functions deep in the call stack -- your LLM call wrapper, your tool executor, your reflection module, etc. Don't pass it as a parameter through every function signature. Use <code>contextvars</code>.</p>
<p>Create a small plumbing module:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># observability.py
import contextvars
from typing import Any, Optional

_current_task = contextvars.ContextVar(&quot;hiveloop_task&quot;, default=None)
_current_agent = contextvars.ContextVar(&quot;hiveloop_agent&quot;, default=None)

def set_current_task(task: Any) -&gt; None:
    _current_task.set(task)

def get_current_task() -&gt; Optional[Any]:
    return _current_task.get()

def clear_current_task() -&gt; None:
    _current_task.set(None)

def set_hiveloop_agent(agent: Any) -&gt; None:
    _current_agent.set(agent)

def get_hiveloop_agent() -&gt; Optional[Any]:
    return _current_agent.get()

def clear_hiveloop_agent() -&gt; None:
    _current_agent.set(None)
</code></pre></div>
<p><strong>Why two contextvars?</strong> They have different lifetimes. The task exists only during one execution. The agent handle persists for the agent's entire lifecycle. Some methods (<code>report_issue</code>, <code>todo</code>, <code>queue_snapshot</code>) are agent-level and work outside any task context. Others (<code>llm_call</code>, <code>plan</code>, <code>escalate</code>) are task-level and require an active task.</p>
<p><strong>Set both</strong> when entering the task context, <strong>clear both</strong> in the <code>finally</code> block. This ensures downstream code can safely call <code>get_current_task()</code> or <code>get_hiveloop_agent()</code> and get <code>None</code> if no context is active.</p>
<h3 id="step-5-track-key-functions-with-agenttrack">Step 5: Track key functions with <code>@agent.track()</code> <a class="heading-anchor" href="#step-5-track-key-functions-with-agenttrack" aria-label="Link to this section">#</a></h3>
<p>For functions whose names are known at definition time (not dynamically dispatched tools), use the <code>@agent.track()</code> decorator:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">@hiveloop_agent.track(&quot;reflect&quot;)
def reflect(self, context):
    # ... reflection logic ...
</code></pre></div>
<p>This adds action nodes to the task timeline. <strong>The &quot;5-7 nodes&quot; rule:</strong> Track 5-7 high-value functions, not 30. Good candidates are functions that call external services, make decisions, take &gt;100ms, or could fail independently. Internal utilities, fast helpers, and functions called hundreds of times per task are bad candidates.</p>
<p>If your functions are class methods where the agent handle isn't available at import time, use <code>track_context()</code> instead (covered in Layer 2b).</p>
<h3 id="what-you-see-on-the-dashboard-after-layer-1">What you see on the dashboard after Layer 1 <a class="heading-anchor" href="#what-you-see-on-the-dashboard-after-layer-1" aria-label="Link to this section">#</a></h3>
<ul>
<li><strong>Task Table</strong> populates with rows: task ID, agent, type, status, duration</li>
<li><strong>Timeline</strong> shows task start/end with action nodes in between</li>
<li><strong>Stats Ribbon</strong> shows: Processing count, Success Rate, Avg Duration, Errors</li>
<li><strong>Activity Stream</strong> shows <code>task_started</code>, <code>task_completed</code>, <code>task_failed</code>, and action events</li>
<li><strong>Agent cards</strong> show PROCESSING badge and current task ID during execution</li>
</ul>
<hr />
<h2 id="layer-2a-llm-call-tracking">Layer 2a: LLM Call Tracking <a class="heading-anchor" href="#layer-2a-llm-call-tracking" aria-label="Link to this section">#</a></h2>
<p><strong>Goal:</strong> Get cost visibility, token usage, and model breakdown.</p>
<h3 id="step-6-find-all-llm-call-sites">Step 6: Find all LLM call sites <a class="heading-anchor" href="#step-6-find-all-llm-call-sites" aria-label="Link to this section">#</a></h3>
<p>Search your codebase for every place an LLM API is called. Common patterns:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># Direct API calls
response = client.chat.completions.create(...)
response = client.messages.create(...)

# Framework wrappers
response = llm_client.complete(...)
response = llm_client.complete_with_tools(...)
response = llm_client.complete_json(...)
</code></pre></div>
<p>Build a catalog. A typical agentic framework has 3-8 LLM call sites:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>#</th>
  <th>Location</th>
  <th>Purpose</th>
  <th>Example name</th>
</tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td>Main loop - reasoning</td>
  <td>Agent decides what to do</td>
  <td><code>&quot;reasoning&quot;</code></td>
</tr>
<tr>
  <td>2</td>
  <td>Main loop - tool use</td>
  <td>Agent generates tool parameters</td>
  <td><code>&quot;tool_use&quot;</code></td>
</tr>
<tr>
  <td>3</td>
  <td>Reflection module</td>
  <td>Agent evaluates its progress</td>
  <td><code>&quot;reflection&quot;</code></td>
</tr>
<tr>
  <td>4</td>
  <td>Planning module</td>
  <td>Agent creates execution plan</td>
  <td><code>&quot;create_plan&quot;</code></td>
</tr>
<tr>
  <td>5</td>
  <td>Summary/compression</td>
  <td>Context window management</td>
  <td><code>&quot;context_compaction&quot;</code></td>
</tr>
<tr>
  <td>6</td>
  <td>Classification/routing</td>
  <td>Routing to sub-agents</td>
  <td><code>&quot;classify&quot;</code></td>
</tr>
</tbody>
</table></div>
<h3 id="step-7-determine-token-extraction-for-each-site">Step 7: Determine token extraction for each site <a class="heading-anchor" href="#step-7-determine-token-extraction-for-each-site" aria-label="Link to this section">#</a></h3>
<p>Different LLM clients expose tokens differently. Inspect each response object:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># Discovery helper -- run once per client type
for attr in dir(response):
    if any(k in attr.lower() for k in ['token', 'usage', 'cost', 'model']):
        print(f&quot;response.{attr} = {getattr(response, attr, '?')}&quot;)
</code></pre></div>
<p>Common patterns:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Pattern</th>
  <th>Example</th>
  <th>How to extract</th>
</tr>
</thead>
<tbody>
<tr>
  <td>On response object</td>
  <td>Anthropic SDK</td>
  <td><code>response.usage.input_tokens</code>, <code>response.usage.output_tokens</code></td>
</tr>
<tr>
  <td>On client after call</td>
  <td>Some wrappers</td>
  <td><code>client._last_input_tokens</code>, <code>client._last_output_tokens</code></td>
</tr>
<tr>
  <td>In response metadata</td>
  <td>LangChain</td>
  <td><code>response.response_metadata['token_usage']</code></td>
</tr>
<tr>
  <td>Not available</td>
  <td>Some wrappers</td>
  <td>Send <code>name</code> and <code>model</code> only -- tokens are optional</td>
</tr>
</tbody>
</table></div>
<h3 id="step-8-add-taskllm_call-at-each-site">Step 8: Add <code>task.llm_call()</code> at each site <a class="heading-anchor" href="#step-8-add-taskllm_call-at-each-site" aria-label="Link to this section">#</a></h3>
<p>The standard pattern:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">import time
from your_app.observability import get_current_task

# 1. Time the call
_start = time.perf_counter()
response = llm_client.complete(prompt=prompt, system=system, max_tokens=2048)
_elapsed_ms = (time.perf_counter() - _start) * 1000

# 2. Extract tokens (adapt to your response shape)
tokens_in = response.usage.input_tokens
tokens_out = response.usage.output_tokens

# 3. Report to HiveLoop
_task = get_current_task()
if _task:
    try:
        _task.llm_call(
            &quot;reasoning&quot;,                      # descriptive name (not the model name)
            model=response.model,             # model identifier
            tokens_in=tokens_in,
            tokens_out=tokens_out,
            cost=estimate_cost(response.model, tokens_in, tokens_out),
            duration_ms=round(_elapsed_ms),
        )
    except Exception:
        pass
</code></pre></div>
<h3 id="cost-estimation-helper">Cost estimation helper <a class="heading-anchor" href="#cost-estimation-helper" aria-label="Link to this section">#</a></h3>
<p>The SDK doesn't calculate costs -- you provide them. Build a lookup table for the models you use:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># Prices as of early 2026 -- update when pricing changes
COST_PER_MILLION = {
    # Anthropic
    &quot;claude-opus-4-6&quot;:              {&quot;input&quot;: 15.00, &quot;output&quot;: 75.00},
    &quot;claude-sonnet-4-5-20250929&quot;:   {&quot;input&quot;: 3.00,  &quot;output&quot;: 15.00},
    &quot;claude-haiku-4-5-20251001&quot;:    {&quot;input&quot;: 0.80,  &quot;output&quot;: 4.00},
    &quot;claude-3-haiku-20240307&quot;:      {&quot;input&quot;: 0.25,  &quot;output&quot;: 1.25},
    # OpenAI
    &quot;gpt-4o&quot;:                       {&quot;input&quot;: 2.50,  &quot;output&quot;: 10.00},
    &quot;gpt-4o-mini&quot;:                  {&quot;input&quot;: 0.15,  &quot;output&quot;: 0.60},
    # Add your models here
}

def estimate_cost(model: str, tokens_in: int, tokens_out: int) -&gt; float | None:
    rates = COST_PER_MILLION.get(model)
    if not rates:
        return None
    return (tokens_in * rates[&quot;input&quot;] / 1_000_000) + (tokens_out * rates[&quot;output&quot;] / 1_000_000)
</code></pre></div>
<h3 id="optional-prompt-and-response-previews">Optional: Prompt and response previews <a class="heading-anchor" href="#optional-prompt-and-response-previews" aria-label="Link to this section">#</a></h3>
<p><code>task.llm_call()</code> supports <code>prompt_preview</code> and <code>response_preview</code> parameters. These make Timeline LLM nodes clickable with real content. But prompts can contain PII, credentials, or internal context -- every string flows through the network and gets stored.</p>
<p><strong>Recommendation:</strong> Don't add them everywhere. Gate them behind a config flag, and truncate aggressively (300 chars max). Only add them to the most opaque call sites where seeing what the agent asked is valuable for debugging:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_task.llm_call(
    &quot;reasoning&quot;,
    model=response.model,
    tokens_in=tokens_in,
    tokens_out=tokens_out,
    cost=estimate_cost(response.model, tokens_in, tokens_out),
    duration_ms=round(_elapsed_ms),
    # Only when config flag is enabled
    prompt_preview=prompt[:300] if config.log_prompts else None,
    response_preview=str(response.content)[:300] if config.log_prompts else None,
)
</code></pre></div>
<p><code>task.llm_call()</code> parameters:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Parameter</th>
  <th>Type</th>
  <th>Required</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>name</code></td>
  <td>str</td>
  <td>yes</td>
  <td>Descriptive label for the Timeline node. Use the <em>purpose</em>, not the model name</td>
</tr>
<tr>
  <td><code>model</code></td>
  <td>str</td>
  <td>yes</td>
  <td>Model identifier. Used for Cost Explorer grouping</td>
</tr>
<tr>
  <td><code>tokens_in</code></td>
  <td>int</td>
  <td>no</td>
  <td>Input token count</td>
</tr>
<tr>
  <td><code>tokens_out</code></td>
  <td>int</td>
  <td>no</td>
  <td>Output token count</td>
</tr>
<tr>
  <td><code>cost</code></td>
  <td>float</td>
  <td>no</td>
  <td>USD cost. If omitted, Cost Explorer won't aggregate</td>
</tr>
<tr>
  <td><code>duration_ms</code></td>
  <td>int</td>
  <td>no</td>
  <td>Wall-clock LLM latency</td>
</tr>
<tr>
  <td><code>prompt_preview</code></td>
  <td>str</td>
  <td>no</td>
  <td>Truncated prompt for debugging. Consider PII implications</td>
</tr>
<tr>
  <td><code>response_preview</code></td>
  <td>str</td>
  <td>no</td>
  <td>Truncated response for debugging</td>
</tr>
<tr>
  <td><code>metadata</code></td>
  <td>dict</td>
  <td>no</td>
  <td>Arbitrary key-value pairs</td>
</tr>
</tbody>
</table></div>
<h3 id="llm-call-metadata-unlocking-advanced-analytics">LLM call metadata: Unlocking advanced analytics <a class="heading-anchor" href="#llm-call-metadata-unlocking-advanced-analytics" aria-label="Link to this section">#</a></h3>
<p>The <code>metadata</code> parameter on <code>task.llm_call()</code> accepts an arbitrary <code>dict</code>. Use it to capture dimensions that power the Insights tab's optimization views — cache efficiency, context window pressure, turn-level cost tracking, and stop reason analysis.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_task = get_current_task()
if _task:
    try:
        _task.llm_call(
            &quot;reasoning&quot;,
            model=response.model,
            tokens_in=tokens_in,
            tokens_out=tokens_out,
            cost=estimate_cost(response.model, tokens_in, tokens_out),
            duration_ms=round(_elapsed_ms),
            metadata={
                # Turn tracking — which turn of the agentic loop is this?
                &quot;turn_number&quot;: turn_count,

                # Cache tokens — how much prompt was cached by the provider?
                &quot;cache_creation_input_tokens&quot;: getattr(
                    response.usage, &quot;cache_creation_input_tokens&quot;, 0
                ),
                &quot;cache_read_input_tokens&quot;: getattr(
                    response.usage, &quot;cache_read_input_tokens&quot;, 0
                ),

                # Stop reason — did the model finish, hit max tokens, or use a tool?
                &quot;stop_reason&quot;: response.stop_reason,  # &quot;end_turn&quot;, &quot;max_tokens&quot;, &quot;tool_use&quot;

                # Context window utilization — how full is the context?
                &quot;context_window_size&quot;: model_context_limit,   # e.g., 200000
                &quot;context_used_tokens&quot;: tokens_in,
                &quot;context_utilization_pct&quot;: round(
                    tokens_in / model_context_limit * 100, 1
                ),

                # Prompt composition — what makes up the input tokens?
                &quot;system_prompt_tokens&quot;: count_tokens(system_prompt),
                &quot;history_tokens&quot;: count_tokens(messages),
                &quot;tool_results_tokens&quot;: count_tokens(tool_results),
            },
        )
    except Exception:
        pass
</code></pre></div>
<p><strong>Field conventions:</strong> Use snake_case keys. The Insights tab looks for these specific field names in metadata:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Metadata field</th>
  <th>Insights tab usage</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>turn_number</code></td>
  <td>Turn-level cost and token charts (Q14)</td>
</tr>
<tr>
  <td><code>cache_read_input_tokens</code></td>
  <td>Cache hit ratio analysis (Q12)</td>
</tr>
<tr>
  <td><code>cache_creation_input_tokens</code></td>
  <td>Cache efficiency trends (Q12)</td>
</tr>
<tr>
  <td><code>stop_reason</code></td>
  <td>Max-token truncation detection (Q20)</td>
</tr>
<tr>
  <td><code>context_utilization_pct</code></td>
  <td>Context pressure monitoring (Q16)</td>
</tr>
<tr>
  <td><code>context_window_size</code></td>
  <td>Context capacity analysis (Q16)</td>
</tr>
<tr>
  <td><code>system_prompt_tokens</code></td>
  <td>Prompt composition breakdown (Q19)</td>
</tr>
<tr>
  <td><code>history_tokens</code></td>
  <td>Prompt growth analysis (Q19)</td>
</tr>
</tbody>
</table></div>
<p><strong>Don't over-instrument.</strong> Start with <code>turn_number</code> and <code>stop_reason</code> — they're the most universally useful. Add cache and context fields only if your LLM provider exposes them. Anthropic's API returns cache token counts directly; OpenAI requires separate calculation.</p>
<h3 id="what-you-see-on-the-dashboard-after-layer-2a">What you see on the dashboard after Layer 2a <a class="heading-anchor" href="#what-you-see-on-the-dashboard-after-layer-2a" aria-label="Link to this section">#</a></h3>
<ul>
<li><strong>Cost Explorer</strong> is fully functional: cost by model, cost by agent, total spend</li>
<li><strong>Task Table</strong> LLM column shows call count, COST column shows dollar amounts</li>
<li><strong>Timeline</strong> has purple LLM nodes with model badges</li>
<li><strong>Stats Ribbon</strong> Cost (1h) populates</li>
<li><strong>Activity Stream</strong> &quot;llm&quot; filter works</li>
</ul>
<hr />
<h2 id="layer-2b-tool-execution-tracking">Layer 2b: Tool Execution Tracking <a class="heading-anchor" href="#layer-2b-tool-execution-tracking" aria-label="Link to this section">#</a></h2>
<p><strong>Goal:</strong> Get tool nodes in the timeline showing what tools the agent used, with inputs and outputs.</p>
<h3 id="the-problem">The problem <a class="heading-anchor" href="#the-problem" aria-label="Link to this section">#</a></h3>
<p>In agentic frameworks, the LLM picks tools at runtime. You can't use <code>@agent.track(&quot;tool_name&quot;)</code> because the tool name isn't known at definition time. <code>track_context()</code> solves this by accepting the name as a runtime parameter.</p>
<h3 id="step-9-wrap-tool-dispatch-with-agenttrack_context">Step 9: Wrap tool dispatch with <code>agent.track_context()</code> <a class="heading-anchor" href="#step-9-wrap-tool-dispatch-with-agenttrack_context" aria-label="Link to this section">#</a></h3>
<p>Find the place in your code where tools are dispatched -- typically a single function that takes a tool name and parameters and routes to the correct implementation.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">from your_app.observability import get_hiveloop_agent

def execute_tool(tool_name, parameters):
    _agent = get_hiveloop_agent()

    if _agent is not None:
        with _agent.track_context(tool_name) as ctx:
            result = tool_registry.execute(tool_name, parameters)
            ctx.set_payload({
                &quot;args&quot;: {k: str(v)[:100] for k, v in parameters.items()},
                &quot;result_preview&quot;: str(result)[:200],
                &quot;success&quot;: result.success,
                &quot;error&quot;: result.error,
            })
        return result
    else:
        return tool_registry.execute(tool_name, parameters)
</code></pre></div>
<p><code>track_context()</code> automatically:</p>
<ul>
<li>Emits <code>action_started</code> when entering the <code>with</code> block</li>
<li>Measures duration</li>
<li>Emits <code>action_completed</code> on clean exit (with the payload you set)</li>
<li>Emits <code>action_failed</code> on exception (capturing exception type and message)</li>
<li>Handles nesting (if one tool calls another, the child becomes a nested node)</li>
</ul>
<h3 id="separated-lifecycle-pattern">Separated lifecycle pattern <a class="heading-anchor" href="#separated-lifecycle-pattern" aria-label="Link to this section">#</a></h3>
<p>If your tool dispatch doesn't use exceptions for error signaling (e.g., it returns a result object with a <code>success</code> flag), separate the <code>__enter__</code> and <code>__exit__</code> calls to ensure the tool always executes exactly once:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_ctx = None
if _agent is not None:
    try:
        _ctx = _agent.track_context(tool_name)
        _ctx.__enter__()
    except Exception:
        _ctx = None

# Tool always executes exactly once, regardless of tracking
result = tool_registry.execute(tool_name, parameters)

if _ctx is not None:
    try:
        _ctx.set_payload({
            &quot;args&quot;: {k: str(v)[:100] for k, v in parameters.items()},
            &quot;result_preview&quot;: (result.output or &quot;&quot;)[:200],
            &quot;success&quot;: result.success,
            &quot;error&quot;: result.error,
        })
        _ctx.__exit__(None, None, None)
    except Exception:
        pass
</code></pre></div>
<p>This avoids the double-execute bug where a naive <code>with</code>-based fallback could run the tool twice if <code>track_context()</code> succeeds but the tool fails.</p>
<h3 id="standardized-tool-payloads-with-tool_payload">Standardized tool payloads with <code>tool_payload()</code> <a class="heading-anchor" href="#standardized-tool-payloads-with-tool_payload" aria-label="Link to this section">#</a></h3>
<p>Building the payload dict for <code>ctx.set_payload()</code> requires deciding which fields to include, how long to let strings grow, and stripping empty values. The <code>tool_payload()</code> helper standardizes this:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">from hiveloop import tool_payload

def execute_tool(tool_name, parameters):
    _agent = get_hiveloop_agent()

    if _agent is not None:
        with _agent.track_context(tool_name) as ctx:
            result = tool_registry.execute(tool_name, parameters)
            ctx.set_payload(tool_payload(
                args=parameters,
                result=result.data,
                success=result.ok,
                error=result.error,
                tool_category=&quot;crm&quot;,
                http_status=result.status_code,
                result_size_bytes=len(result.raw),
            ))
        return result
    else:
        return tool_registry.execute(tool_name, parameters)
</code></pre></div>
<p><code>tool_payload()</code> parameters:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Parameter</th>
  <th>Type</th>
  <th>Default</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>args</code></td>
  <td>dict</td>
  <td><code>None</code></td>
  <td>Tool arguments. Values truncated to <code>args_max_len</code></td>
</tr>
<tr>
  <td><code>result</code></td>
  <td>Any</td>
  <td><code>None</code></td>
  <td>Tool result (stringified &amp; truncated to <code>result_max_len</code>)</td>
</tr>
<tr>
  <td><code>success</code></td>
  <td>bool</td>
  <td><code>True</code></td>
  <td>Whether the call succeeded</td>
</tr>
<tr>
  <td><code>error</code></td>
  <td>str</td>
  <td><code>None</code></td>
  <td>Error message on failure</td>
</tr>
<tr>
  <td><code>duration_ms</code></td>
  <td>int</td>
  <td><code>None</code></td>
  <td>Elapsed milliseconds</td>
</tr>
<tr>
  <td><code>tool_category</code></td>
  <td>str</td>
  <td><code>None</code></td>
  <td>Grouping label (e.g. <code>&quot;crm&quot;</code>, <code>&quot;search&quot;</code>)</td>
</tr>
<tr>
  <td><code>http_status</code></td>
  <td>int</td>
  <td><code>None</code></td>
  <td>HTTP status code for API-backed tools</td>
</tr>
<tr>
  <td><code>result_size_bytes</code></td>
  <td>int</td>
  <td><code>None</code></td>
  <td>Size of the raw result</td>
</tr>
<tr>
  <td><code>args_max_len</code></td>
  <td>int</td>
  <td><code>500</code></td>
  <td>Max chars per arg value before truncation</td>
</tr>
<tr>
  <td><code>result_max_len</code></td>
  <td>int</td>
  <td><code>1000</code></td>
  <td>Max chars for the result string</td>
</tr>
</tbody>
</table></div>
<p>The function strips all <code>None</code>-valued optional fields from the output so payloads stay compact.</p>
<h3 id="what-you-see-on-the-dashboard-after-layer-2b">What you see on the dashboard after Layer 2b <a class="heading-anchor" href="#what-you-see-on-the-dashboard-after-layer-2b" aria-label="Link to this section">#</a></h3>
<ul>
<li><strong>Timeline</strong> has blue tool nodes with names matching the actual tools called</li>
<li>Clicking a tool node shows the payload (args, result preview, success/error)</li>
<li><strong>Activity Stream</strong> shows <code>action_completed</code> and <code>action_failed</code> for each tool call</li>
<li><strong>Failed tools</strong> appear as red nodes with error details</li>
</ul>
<hr />
<h2 id="layer-2c-rich-events">Layer 2c: Rich Events <a class="heading-anchor" href="#layer-2c-rich-events" aria-label="Link to this section">#</a></h2>
<p><strong>Goal:</strong> Plans, escalations, approvals, issues, retries, TODOs, queue state, and schedules.</p>
<p>These are the narrative events that tell the <em>story</em> of what your agent is doing and why. Each one lights up a specific dashboard feature.</p>
<h3 id="plans-and-plan-steps">Plans and plan steps <a class="heading-anchor" href="#plans-and-plan-steps" aria-label="Link to this section">#</a></h3>
<p>If your framework breaks complex tasks into ordered steps (a plan), report the plan structure and step transitions.</p>
<p><strong>When a plan is created:</strong></p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_task = get_current_task()
if _task:
    try:
        step_descriptions = [step.description for step in plan.steps]
        _task.plan(goal=task_description, steps=step_descriptions)
        # Report first step started
        _task.plan_step(step_index=0, action=&quot;started&quot;, summary=plan.steps[0].description)
    except Exception:
        pass
</code></pre></div>
<p><strong>When a step completes and the next one starts:</strong></p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">if _task:
    try:
        _task.plan_step(step_index=current_step.index, action=&quot;completed&quot;,
                        summary=result_summary, turns=current_step.turns_taken)
        _task.plan_step(step_index=next_step.index, action=&quot;started&quot;,
                        summary=next_step.description)
    except Exception:
        pass
</code></pre></div>
<p><strong>When a step fails or is blocked:</strong></p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">if _task:
    try:
        _task.plan_step(step_index=step.index, action=&quot;failed&quot;,
                        summary=f&quot;Blocked: {reason}&quot;)
    except Exception:
        pass
</code></pre></div>
<p><strong>When the plan is revised (replanned):</strong></p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">if _task:
    try:
        _task.plan(goal=revised_goal, steps=new_step_descriptions, revision=revision_count)
        _task.plan_step(step_index=0, action=&quot;started&quot;, summary=new_steps[0].description)
    except Exception:
        pass
</code></pre></div>
<p><strong>Dashboard effect:</strong> A plan progress bar appears above the Timeline: green segments for completed steps, red for failed, gray for pending. This is the fastest way for an operator to see &quot;where did it go wrong?&quot;</p>
<p><code>task.plan()</code> parameters:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Parameter</th>
  <th>Type</th>
  <th>Required</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>goal</code></td>
  <td>str</td>
  <td>yes</td>
  <td>What the plan achieves</td>
</tr>
<tr>
  <td><code>steps</code></td>
  <td>list[str]</td>
  <td>yes</td>
  <td>Ordered step descriptions</td>
</tr>
<tr>
  <td><code>revision</code></td>
  <td>int</td>
  <td>no</td>
  <td>Default 0. Increment on each replan</td>
</tr>
</tbody>
</table></div>
<p><code>task.plan_step()</code> parameters:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Parameter</th>
  <th>Type</th>
  <th>Required</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>step_index</code></td>
  <td>int</td>
  <td>yes</td>
  <td>Zero-based position in the steps list</td>
</tr>
<tr>
  <td><code>action</code></td>
  <td>str</td>
  <td>yes</td>
  <td><code>&quot;started&quot;</code>, <code>&quot;completed&quot;</code>, <code>&quot;failed&quot;</code>, <code>&quot;skipped&quot;</code></td>
</tr>
<tr>
  <td><code>summary</code></td>
  <td>str</td>
  <td>yes</td>
  <td>Outcome description</td>
</tr>
<tr>
  <td><code>turns</code></td>
  <td>int</td>
  <td>no</td>
  <td>LLM turns spent on this step</td>
</tr>
<tr>
  <td><code>tokens</code></td>
  <td>int</td>
  <td>no</td>
  <td>Tokens spent on this step</td>
</tr>
</tbody>
</table></div>
<hr />
<h3 id="escalations">Escalations <a class="heading-anchor" href="#escalations" aria-label="Link to this section">#</a></h3>
<p>When the agent decides it cannot handle something and hands it to a human.</p>
<p><strong>Typical trigger:</strong> Your reflection or self-evaluation module returns a &quot;give up&quot; or &quot;escalate&quot; decision.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">if reflection.decision == &quot;escalate&quot;:
    _task = get_current_task()
    if _task:
        try:
            _task.escalate(
                f&quot;Agent escalated: {reflection.reasoning[:200]}&quot;,
                assigned_to=&quot;human&quot;,
            )
        except Exception:
            pass
</code></pre></div>
<p><strong>Dashboard effect:</strong> Escalation node appears in the Timeline. The agent card may show a warning indicator.</p>
<p><code>task.escalate()</code> parameters:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Parameter</th>
  <th>Type</th>
  <th>Required</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>reason</code></td>
  <td>str</td>
  <td>yes</td>
  <td>Why the agent is escalating</td>
</tr>
<tr>
  <td><code>assigned_to</code></td>
  <td>str</td>
  <td>no</td>
  <td>Who should handle it</td>
</tr>
</tbody>
</table></div>
<hr />
<h3 id="approvals">Approvals <a class="heading-anchor" href="#approvals" aria-label="Link to this section">#</a></h3>
<p>When the agent needs human approval before proceeding.</p>
<p>This is architecturally tricky: the request and the response often happen in different execution contexts (different threads, different HTTP requests). You may not have an active task context when the approval is granted.</p>
<p><strong>Inside a task context</strong> (the standard case):</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_task = get_current_task()
if _task:
    try:
        _task.request_approval(
            f&quot;Approval needed: {event_title}&quot;,
            approver=&quot;human&quot;,
        )
    except Exception:
        pass
</code></pre></div>
<p><strong>When a human approves</strong> (inside a task context):</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_task = get_current_task()
if _task:
    try:
        _task.approval_received(
            f&quot;Event '{event_id}' approved by operator&quot;,
            approved_by=&quot;human&quot;,
            decision=&quot;approved&quot;,
        )
    except Exception:
        pass
</code></pre></div>
<p><strong>When a human rejects</strong> (inside a task context):</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_task = get_current_task()
if _task:
    try:
        _task.approval_received(
            f&quot;Event '{event_id}' dropped by operator&quot;,
            approved_by=&quot;human&quot;,
            decision=&quot;rejected&quot;,
        )
    except Exception:
        pass
</code></pre></div>
<p><strong>Outside a task context</strong> (e.g., approval granted from a separate HTTP handler or queue consumer where no task is active). Use <code>agent.event()</code> to emit the raw event types:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_hl_agent = getattr(get_agent(agent_id), &quot;_hiveloop&quot;, None)
if _hl_agent:
    try:
        _hl_agent.event(
            &quot;approval_requested&quot;,
            payload={&quot;summary&quot;: f&quot;Approval needed: {event_title}&quot;, &quot;approver&quot;: &quot;human&quot;},
        )
    except Exception:
        pass

# Later, when the human responds:
if _hl_agent:
    try:
        _hl_agent.event(
            &quot;approval_received&quot;,
            payload={&quot;summary&quot;: f&quot;Event '{event_id}' approved&quot;, &quot;approved_by&quot;: &quot;human&quot;, &quot;decision&quot;: &quot;approved&quot;},
        )
    except Exception:
        pass
</code></pre></div>
<p><strong>Dashboard effect:</strong> Agent card shows WAITING badge. The Pipeline tab shows the pending approval. After resolution, the approval decision appears in the Activity Stream.</p>
<hr />
<h3 id="issues">Issues <a class="heading-anchor" href="#issues" aria-label="Link to this section">#</a></h3>
<p>Persistent problems the agent encounters -- broken credentials, API failures, configuration errors. Unlike tool failures (which are per-execution), issues represent ongoing operational problems.</p>
<p><strong>When the agent detects a persistent problem:</strong></p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_agent = get_hiveloop_agent()
if _agent:
    try:
        _agent.report_issue(
            summary=&quot;CRM API returning 403&quot;,
            severity=&quot;high&quot;,                   # critical, high, medium, low
            category=&quot;permissions&quot;,            # permissions, connectivity, configuration, etc.
            issue_id=&quot;crm-api-403&quot;,            # stable ID for deduplication
            context={&quot;api&quot;: &quot;crm&quot;, &quot;status_code&quot;: 403},
            occurrence_count=error_count,
        )
    except Exception:
        pass
</code></pre></div>
<p><strong>When the issue is resolved</strong> (human action or automatic recovery):</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">if _agent:
    try:
        _agent.resolve_issue(
            summary=&quot;CRM API recovered&quot;,
            issue_id=&quot;crm-api-403&quot;,
        )
    except Exception:
        pass
</code></pre></div>
<p><strong>Important:</strong> Always use <code>issue_id</code> for recurring issues. Without it, the SDK deduplicates on <code>summary</code> text. If the summary contains timestamps or variable data, each occurrence looks like a new issue.</p>
<p><strong>Dashboard effect:</strong> Agent card shows a red issue badge with count. The Pipeline tab lists open issues by severity. Resolving an issue removes the badge.</p>
<p><code>agent.report_issue()</code> parameters:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Parameter</th>
  <th>Type</th>
  <th>Required</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>summary</code></td>
  <td>str</td>
  <td>yes</td>
  <td>Human-readable description</td>
</tr>
<tr>
  <td><code>severity</code></td>
  <td>str</td>
  <td>yes</td>
  <td><code>&quot;critical&quot;</code>, <code>&quot;high&quot;</code>, <code>&quot;medium&quot;</code>, <code>&quot;low&quot;</code></td>
</tr>
<tr>
  <td><code>category</code></td>
  <td>str</td>
  <td>no</td>
  <td><code>&quot;permissions&quot;</code>, <code>&quot;connectivity&quot;</code>, <code>&quot;configuration&quot;</code>, <code>&quot;data_quality&quot;</code>, <code>&quot;rate_limit&quot;</code>, <code>&quot;other&quot;</code></td>
</tr>
<tr>
  <td><code>issue_id</code></td>
  <td>str</td>
  <td>no</td>
  <td>Stable ID for lifecycle tracking and dedup</td>
</tr>
<tr>
  <td><code>context</code></td>
  <td>dict</td>
  <td>no</td>
  <td>Debugging data (API name, status code, etc.)</td>
</tr>
<tr>
  <td><code>occurrence_count</code></td>
  <td>int</td>
  <td>no</td>
  <td>How many times the agent has seen this</td>
</tr>
</tbody>
</table></div>
<p>Severity guidelines:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Severity</th>
  <th>When to use</th>
  <th>Example</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>critical</code></td>
  <td>Agent cannot function at all</td>
  <td>No API key configured</td>
</tr>
<tr>
  <td><code>high</code></td>
  <td>Major capability degraded</td>
  <td>CRM API returning 403</td>
</tr>
<tr>
  <td><code>medium</code></td>
  <td>Output quality reduced</td>
  <td>Fallback model in use</td>
</tr>
<tr>
  <td><code>low</code></td>
  <td>Informational</td>
  <td>Cache miss rate high</td>
</tr>
</tbody>
</table></div>
<hr />
<h3 id="log-forwarding-with-hiveboardloghandler">Log forwarding with <code>HiveBoardLogHandler</code> <a class="heading-anchor" href="#log-forwarding-with-hiveboardloghandler" aria-label="Link to this section">#</a></h3>
<p>Every Python project uses <code>logging</code>. Instead of manually calling <code>agent.report_issue()</code> at every log site, attach a <code>HiveBoardLogHandler</code> and let WARNING+ log records flow to HiveBoard automatically.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">from hiveloop.contrib.log_handler import HiveBoardLogHandler

agent = hb.agent(&quot;my-agent&quot;, ...)
logging.getLogger(&quot;my_app&quot;).addHandler(HiveBoardLogHandler(agent))
</code></pre></div>
<p><strong>Level mapping:</strong> WARNING → <code>medium</code>, ERROR → <code>high</code>, CRITICAL → <code>critical</code>. INFO and below are ignored by default (configurable via <code>level</code>).</p>
<p><strong>Deduplication:</strong> Each log record gets a stable <code>issue_id</code> of <code>f&quot;log-{logger_name}-{level_name}&quot;</code>, so repeated warnings from the same logger deduplicate naturally on the dashboard.</p>
<p><strong>Context:</strong> Each forwarded record includes <code>logger</code>, <code>filename</code>, <code>lineno</code>, and <code>funcName</code> in the issue context dict, so operators can jump straight to the source.</p>
<p><strong>Safety:</strong> <code>emit()</code> never raises — it inherits the SDK's safety contract.</p>
<p><code>HiveBoardLogHandler</code> constructor parameters:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Parameter</th>
  <th>Type</th>
  <th>Default</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>agent</code></td>
  <td>Agent</td>
  <td>required</td>
  <td>The HiveLoop agent handle from <code>hb.agent()</code></td>
</tr>
<tr>
  <td><code>level</code></td>
  <td>int</td>
  <td><code>logging.WARNING</code></td>
  <td>Minimum log level to forward</td>
</tr>
<tr>
  <td><code>category</code></td>
  <td>str</td>
  <td><code>&quot;log&quot;</code></td>
  <td>Issue category string</td>
</tr>
</tbody>
</table></div>
<hr />
<h3 id="retries">Retries <a class="heading-anchor" href="#retries" aria-label="Link to this section">#</a></h3>
<p>When a failed task creates a follow-up for retry.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_task = get_current_task()
if _task:
    try:
        _task.retry(
            f&quot;Failed ({status}): {message[:100]}&quot;,
            attempt=1,
        )
    except Exception:
        pass
</code></pre></div>
<p><code>task.retry()</code> parameters:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Parameter</th>
  <th>Type</th>
  <th>Required</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>reason</code></td>
  <td>str</td>
  <td>yes</td>
  <td>Why the retry is happening</td>
</tr>
<tr>
  <td><code>attempt</code></td>
  <td>int</td>
  <td>yes</td>
  <td>1-based attempt number</td>
</tr>
<tr>
  <td><code>backoff_seconds</code></td>
  <td>float</td>
  <td>no</td>
  <td>Wait time before next attempt</td>
</tr>
</tbody>
</table></div>
<h4 id="retry-patterns-for-common-failure-modes">Retry patterns for common failure modes <a class="heading-anchor" href="#retry-patterns-for-common-failure-modes" aria-label="Link to this section">#</a></h4>
<p><strong>Structured output parse failure:</strong> When the LLM returns malformed JSON or doesn't follow the required schema, retry with the error message fed back into the prompt.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">MAX_PARSE_RETRIES = 3

for attempt in range(1, MAX_PARSE_RETRIES + 1):
    response = llm_client.complete(prompt=prompt, system=system)

    try:
        result = json.loads(response.content)
        validate_schema(result)
        break  # success
    except (json.JSONDecodeError, ValidationError) as e:
        _task = get_current_task()
        if _task:
            try:
                _task.retry(
                    f&quot;Parse failure (attempt {attempt}): {type(e).__name__}: {str(e)[:100]}&quot;,
                    attempt=attempt,
                )
            except Exception:
                pass

        if attempt == MAX_PARSE_RETRIES:
            raise  # all retries exhausted

        # Feed the error back into the prompt for the next attempt
        prompt += f&quot;\n\nYour previous response was invalid: {e}. Please fix and try again.&quot;
</code></pre></div>
<p><strong>API rate limit / transient failure:</strong> When an external API returns 429 or 5xx.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">for attempt in range(1, 4):
    try:
        result = external_api.call(params)
        break
    except RateLimitError as e:
        backoff = 2 ** attempt  # 2s, 4s, 8s
        _task = get_current_task()
        if _task:
            try:
                _task.retry(
                    f&quot;Rate limited by {api_name}: {e}&quot;,
                    attempt=attempt,
                    backoff_seconds=backoff,
                )
            except Exception:
                pass
        time.sleep(backoff)
</code></pre></div>
<p><strong>Dashboard effect:</strong> Retry events appear as nodes in the Timeline. The Insights tab aggregates retry rates per agent and per failure type, helping identify agents with chronic parse failures or unreliable external dependencies.</p>
<hr />
<h3 id="todos">TODOs <a class="heading-anchor" href="#todos" aria-label="Link to this section">#</a></h3>
<p>Agent-managed work items -- retries, follow-ups, pending actions.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_agent = get_hiveloop_agent()
if _agent:
    try:
        _agent.todo(
            todo_id=&quot;td_001&quot;,
            action=&quot;created&quot;,           # created, completed, failed, dismissed, deferred
            summary=&quot;Retry: Process lead TechNova&quot;,
            priority=&quot;high&quot;,            # optional
            source=&quot;failed_run&quot;,        # optional: who/what created it
            context=&quot;error=timeout&quot;,    # optional: extra detail
        )
    except Exception:
        pass
</code></pre></div>
<p>Report at every lifecycle point: when a TODO is created, when it's completed, when it's dismissed.</p>
<p><strong>Dashboard effect:</strong> The Pipeline tab shows active TODOs. Completed/dismissed TODOs clear from the list.</p>
<hr />
<h3 id="queue-snapshots">Queue snapshots <a class="heading-anchor" href="#queue-snapshots" aria-label="Link to this section">#</a></h3>
<p>If your agents have a work queue (inbox, event queue, task queue), report its state so operators can see if work is piling up.</p>
<p><strong>Best approach:</strong> Use the <code>queue_provider</code> callback at agent registration. The SDK calls this callback every heartbeat cycle (30s) and includes the result in the heartbeat payload automatically.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">def make_queue_provider(agent_id):
    def provider():
        queue = get_queue_for_agent(agent_id)
        if not queue:
            return {&quot;depth&quot;: 0}
        return {
            &quot;depth&quot;: len(queue),
            &quot;oldest_age_seconds&quot;: queue.oldest_age_seconds(),
            &quot;items&quot;: [
                {
                    &quot;id&quot;: item.id,
                    &quot;priority&quot;: item.priority,
                    &quot;source&quot;: item.source,
                    &quot;summary&quot;: item.title or item.message[:80],
                    &quot;queued_at&quot;: item.timestamp.isoformat(),
                }
                for item in queue[:10]  # cap at 10 items
            ],
            &quot;processing&quot;: {
                &quot;id&quot;: current.id,
                &quot;summary&quot;: current.title or current.message[:80],
                &quot;started_at&quot;: current.timestamp.isoformat(),
            } if (current := get_currently_processing(agent_id)) else None,
        }
    return provider

# At agent registration
hiveloop_agent = hb.agent(
    agent_id=&quot;sales&quot;,
    # ... other params ...
    queue_provider=make_queue_provider(&quot;sales&quot;),
)
</code></pre></div>
<p><strong>Note:</strong> The <code>queue_provider</code> is called lazily by the SDK on each heartbeat. It can safely reference runtime state that doesn't exist at registration time (like a runtime that starts later). Just return <code>{&quot;depth&quot;: 0}</code> if the state isn't available yet.</p>
<p><strong>Alternative: Manual snapshots with <code>agent.queue_snapshot()</code></strong></p>
<p>If the callback pattern doesn't fit your architecture, call <code>agent.queue_snapshot()</code> directly at any point:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_agent = get_hiveloop_agent()
if _agent:
    try:
        _agent.queue_snapshot(
            depth=len(queue),
            oldest_age_seconds=queue.oldest_age_seconds(),
            items=[{&quot;id&quot;: i.id, &quot;summary&quot;: i.title[:80]} for i in queue[:10]],
            processing={&quot;id&quot;: current.id, &quot;summary&quot;: current.title[:80]} if current else None,
        )
    except Exception:
        pass
</code></pre></div>
<p>This is useful when queue state changes at irregular intervals (e.g., after each enqueue/dequeue) rather than on a fixed heartbeat schedule.</p>
<p><strong>Dashboard effect:</strong> Queue depth badge (Q:8) on agent cards. The Pipeline tab shows queue contents. Operators can see if work is piling up.</p>
<hr />
<h3 id="scheduled-work">Scheduled work <a class="heading-anchor" href="#scheduled-work" aria-label="Link to this section">#</a></h3>
<p>If your agents have recurring tasks (periodic syncs, cleanup jobs, heartbeat timers), report them once at startup so the Pipeline tab shows the schedule.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_agent = getattr(agent_obj, &quot;_hiveloop&quot;, None)
if _agent:
    try:
        _agent.scheduled(items=[
            {
                &quot;id&quot;: &quot;hb_crm_sync&quot;,
                &quot;name&quot;: &quot;CRM Sync&quot;,
                &quot;interval&quot;: &quot;10m&quot;,
                &quot;enabled&quot;: True,
                &quot;last_status&quot;: None,
            },
            {
                &quot;id&quot;: &quot;hb_email_check&quot;,
                &quot;name&quot;: &quot;Email Check&quot;,
                &quot;interval&quot;: &quot;15m&quot;,
                &quot;enabled&quot;: True,
                &quot;last_status&quot;: None,
            },
        ])
    except Exception:
        pass
</code></pre></div>
<p><strong>Where to put this:</strong> In your agent start/boot function, after scheduled work is configured.</p>
<p><strong>Update on completion.</strong> After each scheduled job runs, call <code>scheduled()</code> again with <code>last_status</code> set to <code>&quot;success&quot;</code> or <code>&quot;failed&quot;</code> and <code>last_run</code> set to the ISO timestamp. This lets the dashboard show when each job last ran and whether it succeeded.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># After a scheduled job completes
if _agent:
    try:
        _agent.scheduled(items=[
            {
                &quot;id&quot;: &quot;hb_crm_sync&quot;,
                &quot;name&quot;: &quot;CRM Sync&quot;,
                &quot;interval&quot;: &quot;10m&quot;,
                &quot;enabled&quot;: True,
                &quot;last_status&quot;: &quot;success&quot;,
                &quot;last_run&quot;: datetime.utcnow().isoformat() + &quot;Z&quot;,
            },
            # ... other scheduled items unchanged ...
        ])
    except Exception:
        pass
</code></pre></div>
<p><strong>Note for frameworks with built-in schedulers:</strong> If your framework already has a scheduler (e.g., APScheduler, Celery Beat, or a custom timer loop), you already know all the schedule metadata. Just call <code>scheduled()</code> once at boot with the full list, then again after each job runs. You don't need to change how your scheduler works — just report what it already knows.</p>
<hr />
<h2 id="layer-3-advanced-observability-patterns">Layer 3: Advanced Observability Patterns <a class="heading-anchor" href="#layer-3-advanced-observability-patterns" aria-label="Link to this section">#</a></h2>
<p><strong>Goal:</strong> Unlock Insights tab analytics with custom events that capture agent behavior patterns — learning, context management, errors, runtime lifecycle, and configuration.</p>
<p>All patterns in this layer use <code>agent.event()</code> or <code>task.event()</code> — the same generic event methods listed in Layer 2c. The SDK doesn't need any changes. You're simply emitting custom events with specific <code>event_type</code> values and structured payloads.</p>
<p><strong>Key principle:</strong> <code>task.event()</code> is for events that happen <em>during</em> a task (context compaction, parse errors, learning moments). <code>agent.event()</code> is for events that happen <em>outside</em> any task (startup, config changes, session lifecycle).</p>
<hr />
<h3 id="learning-and-self-correction-events">Learning and self-correction events <a class="heading-anchor" href="#learning-and-self-correction-events" aria-label="Link to this section">#</a></h3>
<p>When your agent detects it made a mistake and corrects itself, or when your reflection module identifies a pattern for future improvement, emit a learning event. These power the Insights tab's &quot;self-correction rate&quot; metric.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_task = get_current_task()
if _task:
    try:
        _task.event(&quot;custom&quot;, payload={
            &quot;kind&quot;: &quot;learning&quot;,
            &quot;data&quot;: {
                &quot;trigger&quot;: &quot;reflection&quot;,           # what triggered the learning
                &quot;category&quot;: &quot;tool_selection&quot;,      # what was learned about
                &quot;description&quot;: &quot;Selected search_contacts instead of search_deals&quot;,
                &quot;correction_applied&quot;: True,
                &quot;turn_number&quot;: turn_count,
            },
        })
    except Exception:
        pass
</code></pre></div>
<p><strong>When to emit:</strong> After your reflection module identifies an error and the agent adjusts its approach. Also when the agent backtracks on a plan step or changes strategy mid-task.</p>
<hr />
<h3 id="context-compaction-tracking">Context compaction tracking <a class="heading-anchor" href="#context-compaction-tracking" aria-label="Link to this section">#</a></h3>
<p>When your framework compresses, summarizes, or truncates the conversation history to fit within the context window, emit a compaction event. These power the Insights tab's &quot;context pressure&quot; and &quot;prompt bloat&quot; analysis.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">_task = get_current_task()
if _task:
    try:
        _task.event(&quot;custom&quot;, payload={
            &quot;kind&quot;: &quot;context_compaction&quot;,
            &quot;data&quot;: {
                &quot;tokens_before&quot;: tokens_before_compaction,
                &quot;tokens_after&quot;: tokens_after_compaction,
                &quot;tokens_removed&quot;: tokens_before_compaction - tokens_after_compaction,
                &quot;compression_ratio&quot;: round(tokens_after_compaction / tokens_before_compaction, 2),
                &quot;method&quot;: &quot;summary&quot;,        # &quot;summary&quot;, &quot;truncation&quot;, &quot;sliding_window&quot;
                &quot;turn_number&quot;: turn_count,
                &quot;messages_removed&quot;: num_messages_dropped,
            },
        })
    except Exception:
        pass
</code></pre></div>
<p><strong>Where to put this:</strong> In your context window management function — the code that runs when the prompt is about to exceed the model's context limit.</p>
<hr />
<h3 id="rich-error-context">Rich error context <a class="heading-anchor" href="#rich-error-context" aria-label="Link to this section">#</a></h3>
<p>Tool failures and task failures are already captured by <code>track_context()</code> and <code>task()</code>. But some failures carry important diagnostic context that deserves its own event — the specific API response, the validation error, the state that led to the failure.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># When an external API returns an error with useful diagnostic data
_task = get_current_task()
if _task:
    try:
        _task.event(&quot;custom&quot;, payload={
            &quot;kind&quot;: &quot;error_context&quot;,
            &quot;data&quot;: {
                &quot;error_type&quot;: type(exc).__name__,       # &quot;ValidationError&quot;, &quot;HTTPError&quot;
                &quot;error_message&quot;: str(exc)[:500],
                &quot;api_name&quot;: &quot;crm&quot;,
                &quot;status_code&quot;: response.status_code,
                &quot;response_body&quot;: response.text[:200],
                &quot;retry_eligible&quot;: is_retryable(exc),
                &quot;turn_number&quot;: turn_count,
                &quot;action_name&quot;: current_tool_name,
            },
        })
    except Exception:
        pass
</code></pre></div>
<p><strong>When to emit:</strong> Don't emit this for every error — <code>track_context()</code> already captures tool failures. Emit <code>error_context</code> when you have <em>additional</em> diagnostic information that wouldn't fit in the action payload, or for errors that happen outside a tool call (e.g., prompt construction failures, response parsing failures).</p>
<hr />
<h3 id="runtime-lifecycle-events">Runtime lifecycle events <a class="heading-anchor" href="#runtime-lifecycle-events" aria-label="Link to this section">#</a></h3>
<p>Emit events at key moments in your agent's lifecycle — startup, shutdown, configuration reloads, model switches. These power the Insights tab's &quot;agent lifecycle&quot; timeline and help correlate behavior changes with configuration changes.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># At agent startup
_agent = getattr(agent_obj, &quot;_hiveloop&quot;, None)
if _agent:
    try:
        _agent.event(&quot;custom&quot;, payload={
            &quot;kind&quot;: &quot;runtime&quot;,
            &quot;data&quot;: {
                &quot;event&quot;: &quot;agent_started&quot;,
                &quot;config&quot;: {
                    &quot;model&quot;: agent_obj.config.model,
                    &quot;max_turns&quot;: agent_obj.config.max_turns,
                    &quot;temperature&quot;: agent_obj.config.temperature,
                    &quot;tools_enabled&quot;: [t.name for t in agent_obj.tools],
                },
                &quot;environment&quot;: {
                    &quot;python_version&quot;: sys.version.split()[0],
                    &quot;framework_version&quot;: framework.__version__,
                },
            },
        })
    except Exception:
        pass

# When the model changes mid-session (e.g., fallback to cheaper model)
if _agent:
    try:
        _agent.event(&quot;custom&quot;, payload={
            &quot;kind&quot;: &quot;runtime&quot;,
            &quot;data&quot;: {
                &quot;event&quot;: &quot;model_switched&quot;,
                &quot;from_model&quot;: previous_model,
                &quot;to_model&quot;: new_model,
                &quot;reason&quot;: &quot;cost_optimization&quot;,  # or &quot;rate_limit&quot;, &quot;fallback&quot;, &quot;user_request&quot;
            },
        })
    except Exception:
        pass
</code></pre></div>
<hr />
<h3 id="session-and-memory-operations">Session and memory operations <a class="heading-anchor" href="#session-and-memory-operations" aria-label="Link to this section">#</a></h3>
<p>If your agents maintain persistent memory (vector stores, conversation histories, knowledge bases), emit events when memory is read or written. These help diagnose stale memory issues and track memory utilization.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># After writing to persistent memory
_task = get_current_task()
if _task:
    try:
        _task.event(&quot;custom&quot;, payload={
            &quot;kind&quot;: &quot;memory_op&quot;,
            &quot;data&quot;: {
                &quot;operation&quot;: &quot;write&quot;,              # &quot;read&quot;, &quot;write&quot;, &quot;delete&quot;, &quot;search&quot;
                &quot;store&quot;: &quot;vector_db&quot;,              # &quot;vector_db&quot;, &quot;session_history&quot;, &quot;knowledge_base&quot;
                &quot;key_or_query&quot;: memory_key[:100],
                &quot;result_count&quot;: num_results,       # for reads/searches
                &quot;latency_ms&quot;: round(elapsed_ms),
            },
        })
    except Exception:
        pass
</code></pre></div>
<hr />
<h3 id="configuration-snapshots">Configuration snapshots <a class="heading-anchor" href="#configuration-snapshots" aria-label="Link to this section">#</a></h3>
<p>Emit the full agent configuration at startup and whenever it changes. This lets operators compare &quot;what was the config when this agent was working&quot; vs &quot;what changed when it broke.&quot;</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># At agent boot or after config reload
_agent = getattr(agent_obj, &quot;_hiveloop&quot;, None)
if _agent:
    try:
        _agent.event(&quot;custom&quot;, payload={
            &quot;kind&quot;: &quot;config_snapshot&quot;,
            &quot;data&quot;: {
                &quot;model&quot;: agent_obj.config.model,
                &quot;temperature&quot;: agent_obj.config.temperature,
                &quot;max_turns&quot;: agent_obj.config.max_turns,
                &quot;max_tokens&quot;: agent_obj.config.max_tokens,
                &quot;tools&quot;: [t.name for t in agent_obj.tools],
                &quot;system_prompt_hash&quot;: hashlib.md5(
                    system_prompt.encode()
                ).hexdigest()[:8],
                &quot;system_prompt_tokens&quot;: count_tokens(system_prompt),
                &quot;guardrails_enabled&quot;: agent_obj.config.guardrails_enabled,
                &quot;version&quot;: agent_obj.config.version,
            },
        })
    except Exception:
        pass
</code></pre></div>
<p><strong>When to emit:</strong> At agent startup, and again whenever configuration changes at runtime (model switch, tool list change, temperature adjustment). The <code>system_prompt_hash</code> lets operators detect prompt changes without logging the full prompt.</p>
<hr />
<h2 id="layer-4-client-side-detection-patterns">Layer 4: Client-Side Detection Patterns <a class="heading-anchor" href="#layer-4-client-side-detection-patterns" aria-label="Link to this section">#</a></h2>
<p><strong>Goal:</strong> Detect behavioral anomalies in your agentic loop and report them to HiveBoard.</p>
<p>These patterns require logic that runs <em>in your framework</em>, not in the SDK. The SDK is the reporting channel — you implement the detection, then emit the findings via <code>task.event()</code> or <code>agent.event()</code>. These patterns power the Insights tab's &quot;Smart Detectors&quot; (contradiction detector, silent drop detector, prompt bloat detector, etc.).</p>
<hr />
<h3 id="loop-and-cycle-detection">Loop and cycle detection <a class="heading-anchor" href="#loop-and-cycle-detection" aria-label="Link to this section">#</a></h3>
<p>Agentic loops can get stuck — calling the same tool with the same arguments, or alternating between two tools without making progress. Detect this by tracking recent actions and looking for repetition.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># Cycle detection state — maintain per task
_recent_actions = []  # list of (tool_name, args_hash) tuples
MAX_HISTORY = 20
CYCLE_THRESHOLD = 3   # same action 3+ times = probable loop

def detect_cycle(tool_name: str, arguments: dict) -&gt; bool:
    &quot;&quot;&quot;Returns True if a cycle is detected.&quot;&quot;&quot;
    args_hash = hashlib.md5(
        json.dumps(arguments, sort_keys=True).encode()
    ).hexdigest()[:8]

    _recent_actions.append((tool_name, args_hash))
    if len(_recent_actions) &gt; MAX_HISTORY:
        _recent_actions.pop(0)

    # Check for exact repetition
    recent = _recent_actions[-CYCLE_THRESHOLD:]
    if len(recent) == CYCLE_THRESHOLD and len(set(recent)) == 1:
        return True

    # Check for A-B-A-B alternation
    if len(_recent_actions) &gt;= 4:
        last4 = _recent_actions[-4:]
        if last4[0] == last4[2] and last4[1] == last4[3] and last4[0] != last4[1]:
            return True

    return False

# In your tool dispatch function
def execute_tool(tool_name, arguments):
    if detect_cycle(tool_name, arguments):
        _task = get_current_task()
        if _task:
            try:
                _task.event(&quot;custom&quot;, payload={
                    &quot;kind&quot;: &quot;anomaly&quot;,
                    &quot;data&quot;: {
                        &quot;detector&quot;: &quot;cycle&quot;,
                        &quot;tool_name&quot;: tool_name,
                        &quot;args_hash&quot;: hashlib.md5(
                            json.dumps(arguments, sort_keys=True).encode()
                        ).hexdigest()[:8],
                        &quot;cycle_length&quot;: CYCLE_THRESHOLD,
                        &quot;turn_number&quot;: turn_count,
                        &quot;message&quot;: f&quot;Agent called {tool_name} {CYCLE_THRESHOLD}x with same args&quot;,
                    },
                })
            except Exception:
                pass

        # Optional: break the cycle by failing the task or injecting guidance
        # raise CycleDetectedError(f&quot;Loop detected: {tool_name}&quot;)

    result = tool_registry.execute(tool_name, arguments)
    return result
</code></pre></div>
<p><strong>What to do when a cycle is detected:</strong> Reporting the anomaly is mandatory. Breaking the cycle is your design choice — you can raise an exception to fail the task, inject a system message (&quot;you are repeating yourself&quot;), reduce max_turns, or let it continue but alert the operator.</p>
<hr />
<h3 id="prompt-composition-breakdown">Prompt composition breakdown <a class="heading-anchor" href="#prompt-composition-breakdown" aria-label="Link to this section">#</a></h3>
<p>Track what percentage of the context window is consumed by each component: system prompt, conversation history, tool results, and user message. This detects &quot;prompt bloat&quot; — when one component grows to dominate the context window.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">def analyze_prompt_composition(
    system_prompt: str,
    messages: list,
    tool_results: list | None = None,
    model_context_limit: int = 200_000,
) -&gt; dict:
    &quot;&quot;&quot;Analyze token distribution across prompt components.&quot;&quot;&quot;
    sys_tokens = count_tokens(system_prompt)
    history_tokens = count_tokens(messages)
    tool_tokens = count_tokens(tool_results) if tool_results else 0
    total = sys_tokens + history_tokens + tool_tokens

    breakdown = {
        &quot;system_prompt_tokens&quot;: sys_tokens,
        &quot;history_tokens&quot;: history_tokens,
        &quot;tool_results_tokens&quot;: tool_tokens,
        &quot;total_input_tokens&quot;: total,
        &quot;context_utilization_pct&quot;: round(total / model_context_limit * 100, 1),
        &quot;largest_component&quot;: max(
            [(&quot;system&quot;, sys_tokens), (&quot;history&quot;, history_tokens), (&quot;tools&quot;, tool_tokens)],
            key=lambda x: x[1],
        )[0],
    }

    # Detect bloat: any single component &gt; 60% of total
    for name, tokens in [(&quot;system&quot;, sys_tokens), (&quot;history&quot;, history_tokens), (&quot;tools&quot;, tool_tokens)]:
        pct = (tokens / total * 100) if total &gt; 0 else 0
        breakdown[f&quot;{name}_pct&quot;] = round(pct, 1)
        if pct &gt; 60:
            breakdown[&quot;bloat_warning&quot;] = f&quot;{name} is {pct:.0f}% of prompt&quot;

    return breakdown

# Before each LLM call
composition = analyze_prompt_composition(system_prompt, messages, tool_results)

_task = get_current_task()
if _task and composition.get(&quot;context_utilization_pct&quot;, 0) &gt; 50:
    try:
        _task.event(&quot;custom&quot;, payload={
            &quot;kind&quot;: &quot;prompt_composition&quot;,
            &quot;data&quot;: {
                **composition,
                &quot;turn_number&quot;: turn_count,
            },
        })
    except Exception:
        pass
</code></pre></div>
<p><strong>When to emit:</strong> Every turn is too noisy. Emit when context utilization exceeds 50%, or when the composition changes significantly (e.g., tool results jump from 10% to 40% of the prompt). The Insights tab aggregates these to show how prompt composition evolves across turns within a task.</p>
<hr />
<h3 id="state-mutation-tracking">State mutation tracking <a class="heading-anchor" href="#state-mutation-tracking" aria-label="Link to this section">#</a></h3>
<p>If your agents maintain mutable state (working memory, scratchpads, accumulated results), track significant mutations. This helps diagnose &quot;the agent forgot what it learned&quot; or &quot;the state silently changed.&quot;</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">import copy

class StateMutationTracker:
    &quot;&quot;&quot;Track changes to agent working state and report significant mutations.&quot;&quot;&quot;

    def __init__(self):
        self._previous_state = {}
        self._mutation_count = 0

    def check_mutation(self, current_state: dict, turn_number: int) -&gt; dict | None:
        &quot;&quot;&quot;Compare current state to previous. Returns diff if changed.&quot;&quot;&quot;
        if not self._previous_state:
            self._previous_state = copy.deepcopy(current_state)
            return None

        changes = {}
        all_keys = set(list(self._previous_state.keys()) + list(current_state.keys()))

        for key in all_keys:
            old_val = self._previous_state.get(key)
            new_val = current_state.get(key)
            if old_val != new_val:
                changes[key] = {
                    &quot;action&quot;: &quot;added&quot; if old_val is None else &quot;removed&quot; if new_val is None else &quot;modified&quot;,
                    &quot;old_type&quot;: type(old_val).__name__ if old_val is not None else None,
                    &quot;new_type&quot;: type(new_val).__name__ if new_val is not None else None,
                }

        if changes:
            self._mutation_count += 1
            self._previous_state = copy.deepcopy(current_state)
            return {
                &quot;mutation_number&quot;: self._mutation_count,
                &quot;turn_number&quot;: turn_number,
                &quot;keys_changed&quot;: list(changes.keys()),
                &quot;changes&quot;: changes,
                &quot;state_size&quot;: len(current_state),
            }

        return None

# Usage — at the end of each turn
_tracker = StateMutationTracker()  # one per task

def after_turn(agent_state: dict, turn_number: int):
    mutation = _tracker.check_mutation(agent_state, turn_number)
    if mutation:
        _task = get_current_task()
        if _task:
            try:
                _task.event(&quot;custom&quot;, payload={
                    &quot;kind&quot;: &quot;state_mutation&quot;,
                    &quot;data&quot;: mutation,
                })
            except Exception:
                pass
</code></pre></div>
<p><strong>What to track:</strong> Track your agent's working memory, not internal SDK state. Good candidates: accumulated search results, extracted entities, plan progress, user preferences collected during conversation. Don't track conversation history (that's covered by prompt composition) or transient loop variables.</p>
<p><strong>Privacy note:</strong> Don't include actual state <em>values</em> in the mutation event — only keys, types, and whether they were added/modified/removed. State values may contain PII or sensitive data.</p>
<hr />
<h2 id="putting-it-all-together-complete-turn-instrumentation">Putting It All Together: Complete Turn Instrumentation <a class="heading-anchor" href="#putting-it-all-together-complete-turn-instrumentation" aria-label="Link to this section">#</a></h2>
<p>Here's how all layers combine in a single agentic loop turn:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">import time
import json
from your_app.observability import get_current_task, get_hiveloop_agent, estimate_cost

def run_turn(messages, tool_catalog, system_prompt):
    &quot;&quot;&quot;One turn of the agentic loop with full HiveLoop instrumentation.&quot;&quot;&quot;

    # --- LLM CALL (Layer 2a) ---
    _start = time.perf_counter()
    response = llm_client.chat(
        messages=messages,
        tools=tool_catalog,
        system=system_prompt,
    )
    _elapsed_ms = (time.perf_counter() - _start) * 1000

    tokens_in = response.usage.input_tokens
    tokens_out = response.usage.output_tokens

    _task = get_current_task()
    if _task:
        try:
            _task.llm_call(
                &quot;agent_turn&quot;,
                model=response.model,
                tokens_in=tokens_in,
                tokens_out=tokens_out,
                cost=estimate_cost(response.model, tokens_in, tokens_out),
                duration_ms=round(_elapsed_ms),
            )
        except Exception:
            pass

    # --- TOOL EXECUTION (Layer 2b) ---
    if response.tool_calls:
        _agent = get_hiveloop_agent()
        for tool_call in response.tool_calls:
            if _agent is not None:
                with _agent.track_context(tool_call.name) as ctx:
                    result = tool_registry.execute(tool_call.name, tool_call.arguments)
                    ctx.set_payload({
                        &quot;args&quot;: {k: str(v)[:100] for k, v in tool_call.arguments.items()},
                        &quot;result_preview&quot;: str(result)[:200],
                        &quot;success&quot;: result.success,
                    })
            else:
                result = tool_registry.execute(tool_call.name, tool_call.arguments)

            messages.append({&quot;role&quot;: &quot;tool&quot;, &quot;content&quot;: str(result)})

    return response
</code></pre></div>
<hr />
<h2 id="architecture-patterns">Architecture Patterns <a class="heading-anchor" href="#architecture-patterns" aria-label="Link to this section">#</a></h2>
<h3 id="single-agent-system">Single-agent system <a class="heading-anchor" href="#single-agent-system" aria-label="Link to this section">#</a></h3>
<p>The simplest case. One agent, one <code>hb.agent()</code> call, one task context at a time.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">hb = hiveloop.init(api_key=&quot;...&quot;, endpoint=&quot;...&quot;)
agent = hb.agent(agent_id=&quot;main&quot;, ...)

while True:
    message = queue.get()
    with agent.task(f&quot;task-{uuid.uuid4().hex[:8]}&quot;, project=&quot;my-app&quot;) as task:
        set_current_task(task)
        set_hiveloop_agent(agent)
        try:
            process(message)
        finally:
            clear_current_task()
            clear_hiveloop_agent()
</code></pre></div>
<h3 id="multi-agent-system">Multi-agent system <a class="heading-anchor" href="#multi-agent-system" aria-label="Link to this section">#</a></h3>
<p>Each agent gets its own <code>hb.agent()</code> handle. The contextvars ensure that concurrent agents (in threads or async tasks) don't cross-contaminate their telemetry.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">hb = hiveloop.init(api_key=&quot;...&quot;, endpoint=&quot;...&quot;)

for agent_config in config.agents:
    agent = create_agent(agent_config)
    agent._hiveloop = hb.agent(
        agent_id=agent_config.id,
        type=agent_config.role,
        version=agent_config.model,
        framework=&quot;my-framework&quot;,
    )
</code></pre></div>
<h3 id="api-driven-agents-fastapi-flask">API-driven agents (FastAPI, Flask) <a class="heading-anchor" href="#api-driven-agents-fastapi-flask" aria-label="Link to this section">#</a></h3>
<p>Each HTTP request is a task. Set the task context in middleware or at the handler level.</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">@app.post(&quot;/agents/{agent_id}/run&quot;)
async def run_agent(agent_id: str, body: RunRequest):
    agent = get_agent(agent_id)
    _hl = getattr(agent, &quot;_hiveloop&quot;, None)

    if _hl:
        with _hl.task(f&quot;{agent_id}-{uuid.uuid4().hex[:8]}&quot;, project=&quot;my-app&quot;) as task:
            set_current_task(task)
            set_hiveloop_agent(_hl)
            try:
                result = agent.run(body.message)
            finally:
                clear_current_task()
                clear_hiveloop_agent()
    else:
        result = agent.run(body.message)

    return {&quot;response&quot;: result}
</code></pre></div>
<h3 id="framework-callbacks-langchain-crewai-autogen">Framework callbacks (LangChain, CrewAI, AutoGen) <a class="heading-anchor" href="#framework-callbacks-langchain-crewai-autogen" aria-label="Link to this section">#</a></h3>
<p>If your framework provides callback hooks, wire HiveLoop into them:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python"># LangChain example
class HiveLoopCallbackHandler(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        self._llm_start = time.perf_counter()

    def on_llm_end(self, response, **kwargs):
        elapsed = (time.perf_counter() - self._llm_start) * 1000
        _task = get_current_task()
        if _task:
            _task.llm_call(&quot;llm_call&quot;, model=response.model,
                          tokens_in=response.usage.input_tokens,
                          tokens_out=response.usage.output_tokens,
                          duration_ms=round(elapsed))

    def on_tool_start(self, serialized, input_str, **kwargs):
        self._tool_ctx = get_hiveloop_agent().track_context(serialized[&quot;name&quot;])
        self._tool_ctx.__enter__()

    def on_tool_end(self, output, **kwargs):
        if self._tool_ctx:
            self._tool_ctx.set_payload({&quot;result_preview&quot;: str(output)[:200]})
            self._tool_ctx.__exit__(None, None, None)
</code></pre></div>
<hr />
<h2 id="incremental-adoption-strategy">Incremental Adoption Strategy <a class="heading-anchor" href="#incremental-adoption-strategy" aria-label="Link to this section">#</a></h2>
<p>Don't do everything at once. Ship in tiers:</p>
<h3 id="tier-1-highest-value-lowest-effort">Tier 1 (highest value, lowest effort) <a class="heading-anchor" href="#tier-1-highest-value-lowest-effort" aria-label="Link to this section">#</a></h3>
<ul>
<li><code>hiveloop.init()</code> + <code>hb.agent()</code> (Layer 0)</li>
<li><code>agent.task()</code> wrapping (Layer 1)</li>
<li><code>task.llm_call()</code> at your main LLM call sites (Layer 2a)</li>
<li><code>agent.report_issue()</code> at persistent error detection points (Layer 2c)</li>
</ul>
<p><strong>Result:</strong> Agent cards with heartbeats, task table with durations, cost explorer, issue badges. This alone answers &quot;are my agents running?&quot;, &quot;how much are they costing?&quot;, and &quot;is anything broken?&quot;</p>
<h3 id="tier-2-unlocks-debugging">Tier 2 (unlocks debugging) <a class="heading-anchor" href="#tier-2-unlocks-debugging" aria-label="Link to this section">#</a></h3>
<ul>
<li><code>agent.track_context()</code> at tool dispatch (Layer 2b)</li>
<li><code>task.plan()</code> + <code>task.plan_step()</code> if your framework uses planning (Layer 2c)</li>
<li><code>task.escalate()</code> at escalation decision points (Layer 2c)</li>
</ul>
<p><strong>Result:</strong> Full timeline with tool nodes, plan progress bars, escalation tracking. Now operators can answer &quot;what went wrong in this specific run?&quot;</p>
<h3 id="tier-3-unlocks-operations">Tier 3 (unlocks operations) <a class="heading-anchor" href="#tier-3-unlocks-operations" aria-label="Link to this section">#</a></h3>
<ul>
<li><code>queue_provider</code> callback for queue snapshots (Layer 2c)</li>
<li>Approval request/received events (Layer 2c)</li>
<li><code>task.retry()</code> at retry points (Layer 2c)</li>
</ul>
<p><strong>Result:</strong> Queue depth visibility, WAITING badges, retry tracking. Operators can answer &quot;is work piling up?&quot; and &quot;does anything need my attention?&quot;</p>
<h3 id="tier-4-polish">Tier 4 (polish) <a class="heading-anchor" href="#tier-4-polish" aria-label="Link to this section">#</a></h3>
<ul>
<li><code>agent.todo()</code> at TODO lifecycle points (Layer 2c)</li>
<li><code>agent.scheduled()</code> at agent startup (Layer 2c)</li>
<li><code>prompt_preview</code>/<code>response_preview</code> on key LLM calls (Layer 2a)</li>
</ul>
<p><strong>Result:</strong> Pipeline tab fully populated. Complete operational picture.</p>
<h3 id="tier-5-advanced-analytics">Tier 5 (advanced analytics) <a class="heading-anchor" href="#tier-5-advanced-analytics" aria-label="Link to this section">#</a></h3>
<ul>
<li>LLM call <code>metadata</code> with cache tokens, turn numbers, stop reasons (Layer 2a)</li>
<li>Context compaction and learning events (Layer 3)</li>
<li>Configuration snapshots at startup (Layer 3)</li>
<li>Loop/cycle detection in tool dispatch (Layer 4)</li>
<li>Prompt composition breakdown (Layer 4)</li>
<li>State mutation tracking (Layer 4)</li>
</ul>
<p><strong>Result:</strong> Insights tab fully powered. Anomaly detection, cost optimization recommendations, context pressure monitoring, and behavioral analysis all active.</p>
<hr />
<h2 id="common-mistakes">Common Mistakes <a class="heading-anchor" href="#common-mistakes" aria-label="Link to this section">#</a></h2>
<ol>
<li><p><strong>Reusing task IDs.</strong> If two executions share a task ID, the dashboard merges them into one row. Always generate unique IDs.</p>
</li>
<li><p><strong>Forgetting the <code>finally</code> block.</strong> If you set <code>set_current_task(task)</code> but don't clear it in <code>finally</code>, a failed task leaves stale context for the next execution.</p>
</li>
<li><p><strong>Tracking too many functions.</strong> 30+ action nodes per task makes the timeline unreadable. Track 5-7 high-value functions.</p>
</li>
<li><p><strong>Reporting issues for every single failure.</strong> Use a threshold (3+ consecutive failures) before calling <code>report_issue()</code>. Individual tool failures are already captured by <code>track_context()</code>.</p>
</li>
<li><p><strong>Not using <code>issue_id</code>.</strong> Without it, the SDK deduplicates on summary text. Timestamps or variable data in the summary defeats dedup.</p>
</li>
<li><p><strong>Forgetting to resolve issues.</strong> An unresolved issue shows a red badge forever. Wire <code>resolve_issue()</code> into your recovery paths.</p>
</li>
<li><p><strong>Calling task-level methods outside task context.</strong> <code>get_current_task()</code> returns <code>None</code> between tasks. Always check before calling. Agent-level methods (<code>report_issue</code>, <code>todo</code>, <code>queue_snapshot</code>, <code>scheduled</code>) work anywhere.</p>
</li>
<li><p><strong>Double-execute bug.</strong> If your <code>with track_context()</code> fallback pattern runs the tool both inside and outside the <code>with</code> block on failure, the tool executes twice. Use the separated <code>__enter__</code>/<code>__exit__</code> pattern instead.</p>
</li>
<li><p><strong>Not passing <code>cost=</code>.</strong> Without it, the Cost Explorer works but shows $0. Always pass <code>cost=estimate_cost(...)</code>.</p>
</li>
<li><p><strong>Plan step indices off by one.</strong> <code>step_index</code> is zero-based. Step 1 in your plan is <code>step_index=0</code>.</p>
</li>
</ol>
<hr />
<h2 id="shutdown">Shutdown <a class="heading-anchor" href="#shutdown" aria-label="Link to this section">#</a></h2>
<p>Call <code>hiveloop.shutdown()</code> when your application exits to flush any remaining buffered events:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">import atexit
atexit.register(hiveloop.shutdown)
</code></pre></div>
<p>Or in your framework's shutdown handler:</p>
<div class="code-block"><span class="code-lang">python</span><pre><code class="language-python">def on_shutdown():
    hiveloop.shutdown(timeout=10)  # waits up to 10s for flush
</code></pre></div>
<hr />
<h2 id="quick-reference-all-sdk-methods">Quick Reference: All SDK Methods <a class="heading-anchor" href="#quick-reference-all-sdk-methods" aria-label="Link to this section">#</a></h2>
<h3 id="module-level">Module-level <a class="heading-anchor" href="#module-level" aria-label="Link to this section">#</a></h3>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Method</th>
  <th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>hiveloop.init(**kwargs)</code></td>
  <td>Initialize SDK (singleton)</td>
</tr>
<tr>
  <td><code>hiveloop.shutdown(timeout=10)</code></td>
  <td>Flush and stop</td>
</tr>
<tr>
  <td><code>hiveloop.flush()</code></td>
  <td>Force flush without stopping</td>
</tr>
<tr>
  <td><code>hiveloop.reset()</code></td>
  <td>Flush, stop, clear singleton (testing)</td>
</tr>
<tr>
  <td><code>hiveloop.tool_payload(**kw)</code></td>
  <td>Build standardized tool payload dict</td>
</tr>
</tbody>
</table></div>
<h3 id="agent-level-hbagent-returns-this">Agent-level (<code>hb.agent()</code> returns this) <a class="heading-anchor" href="#agent-level-hbagent-returns-this" aria-label="Link to this section">#</a></h3>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Method</th>
  <th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>agent.task(task_id, **kw)</code></td>
  <td>Context manager for task tracking</td>
</tr>
<tr>
  <td><code>agent.start_task(task_id, **kw)</code></td>
  <td>Manual task lifecycle (caller must complete/fail)</td>
</tr>
<tr>
  <td><code>agent.track(name)</code></td>
  <td>Decorator for action tracking</td>
</tr>
<tr>
  <td><code>agent.track_context(name)</code></td>
  <td>Context manager for dynamic action tracking</td>
</tr>
<tr>
  <td><code>agent.report_issue(summary, **kw)</code></td>
  <td>Report persistent operational issue</td>
</tr>
<tr>
  <td><code>agent.resolve_issue(summary, **kw)</code></td>
  <td>Resolve a reported issue</td>
</tr>
<tr>
  <td><code>agent.todo(todo_id, action, summary, **kw)</code></td>
  <td>Report TODO lifecycle event</td>
</tr>
<tr>
  <td><code>agent.queue_snapshot(**kw)</code></td>
  <td>Report queue state</td>
</tr>
<tr>
  <td><code>agent.scheduled(items=[...])</code></td>
  <td>Report scheduled work</td>
</tr>
<tr>
  <td><code>agent.event(event_type, payload)</code></td>
  <td>Custom agent-level event</td>
</tr>
<tr>
  <td><code>agent.llm_call(name, model, **kw)</code></td>
  <td>LLM call outside task context</td>
</tr>
</tbody>
</table></div>
<h3 id="task-level-agenttask-yields-this">Task-level (<code>agent.task()</code> yields this) <a class="heading-anchor" href="#task-level-agenttask-yields-this" aria-label="Link to this section">#</a></h3>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Method</th>
  <th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>task.llm_call(name, model, **kw)</code></td>
  <td>Log LLM call within task</td>
</tr>
<tr>
  <td><code>task.plan(goal, steps, **kw)</code></td>
  <td>Declare execution plan</td>
</tr>
<tr>
  <td><code>task.plan_step(step_index, action, summary, **kw)</code></td>
  <td>Update plan step</td>
</tr>
<tr>
  <td><code>task.escalate(reason, **kw)</code></td>
  <td>Escalate to human</td>
</tr>
<tr>
  <td><code>task.request_approval(summary, **kw)</code></td>
  <td>Request human approval</td>
</tr>
<tr>
  <td><code>task.approval_received(summary, **kw)</code></td>
  <td>Record approval decision</td>
</tr>
<tr>
  <td><code>task.retry(reason, **kw)</code></td>
  <td>Record retry attempt</td>
</tr>
<tr>
  <td><code>task.event(event_type, payload)</code></td>
  <td>Custom task-level event</td>
</tr>
<tr>
  <td><code>task.set_payload(dict)</code></td>
  <td>Add payload to completion event</td>
</tr>
<tr>
  <td><code>task.complete(status=&quot;success&quot;, payload=None)</code></td>
  <td>Manual completion (for <code>start_task()</code> flow)</td>
</tr>
<tr>
  <td><code>task.fail(exception=None, payload=None)</code></td>
  <td>Manual failure (for <code>start_task()</code> flow)</td>
</tr>
</tbody>
</table></div>
<h3 id="context-manager-agenttrack_context-yields-this">Context manager (<code>agent.track_context()</code> yields this) <a class="heading-anchor" href="#context-manager-agenttrack_context-yields-this" aria-label="Link to this section">#</a></h3>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Method</th>
  <th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>ctx.set_payload(dict)</code></td>
  <td>Attach metadata to the action_completed event</td>
</tr>
</tbody>
</table></div>
<h3 id="contrib-hiveloopcontrib">Contrib (<code>hiveloop.contrib</code>) <a class="heading-anchor" href="#contrib-hiveloopcontrib" aria-label="Link to this section">#</a></h3>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Class</th>
  <th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>HiveBoardLogHandler(agent, level, category)</code></td>
  <td><code>logging.Handler</code> that forwards WARNING+ to <code>report_issue()</code></td>
</tr>
</tbody>
</table></div>
<h3 id="custom-event-kind-values-layer-3-amp-4">Custom event <code>kind</code> values (Layer 3 &amp; 4) <a class="heading-anchor" href="#custom-event-kind-values-layer-3-amp-4" aria-label="Link to this section">#</a></h3>
<p>Use <code>agent.event(&quot;custom&quot;, payload={&quot;kind&quot;: &quot;...&quot;, &quot;data&quot;: {...}})</code> or <code>task.event(...)</code> with these kinds:</p>
<div class="table-wrapper"><table>
<thead>
<tr>
  <th>Kind</th>
  <th>Layer</th>
  <th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>learning</code></td>
  <td>3</td>
  <td>Self-correction or strategy change detected</td>
</tr>
<tr>
  <td><code>context_compaction</code></td>
  <td>3</td>
  <td>Context window compressed/summarized</td>
</tr>
<tr>
  <td><code>error_context</code></td>
  <td>3</td>
  <td>Rich diagnostic data for a failure</td>
</tr>
<tr>
  <td><code>runtime</code></td>
  <td>3</td>
  <td>Agent startup, shutdown, model switch</td>
</tr>
<tr>
  <td><code>memory_op</code></td>
  <td>3</td>
  <td>Read/write to persistent memory store</td>
</tr>
<tr>
  <td><code>config_snapshot</code></td>
  <td>3</td>
  <td>Full configuration at boot or change</td>
</tr>
<tr>
  <td><code>anomaly</code></td>
  <td>4</td>
  <td>Loop/cycle detected in tool dispatch</td>
</tr>
<tr>
  <td><code>prompt_composition</code></td>
  <td>4</td>
  <td>Token breakdown by prompt component</td>
</tr>
<tr>
  <td><code>state_mutation</code></td>
  <td>4</td>
  <td>Working state keys changed between turns</td>
</tr>
</tbody>
</table></div>

        </article>
        <div class="prev-next"><a class="prev-next-link prev" href="user-manual.html"><span class="prev-next-label">← Previous</span><span class="prev-next-title">SDK Manual</span></a><a class="prev-next-link next" href="docs-dashboard-guide.html"><span class="prev-next-label">Next →</span><span class="prev-next-title">Dashboard Guide</span></a></div>
    </main>

    <!-- RIGHT TOC -->
<nav class="page-toc"><div class="page-toc-title">On this page</div>
<a class="toc-link" href="#prerequisites">Prerequisites</a>
<a class="toc-link" href="#layer-0-initialization-and-agent-registration">Layer 0: Initialization and Agent Registration</a>
<a class="toc-link toc-h3" href="#step-1-initialize-the-sdk">Step 1: Initialize the SDK</a>
<a class="toc-link toc-h3" href="#step-2-register-agents">Step 2: Register agents</a>
<a class="toc-link toc-h3" href="#heartbeat-payload-rich-agent-telemetry">Heartbeat payload: Rich agent telemetry</a>
<a class="toc-link toc-h3" href="#what-you-see-on-the-dashboard-after-layer-0">What you see on the dashboard after Layer 0</a>
<a class="toc-link toc-h3" href="#safety-contract">Safety contract</a>
<a class="toc-link" href="#layer-1-task-boundaries-and-action-tracking">Layer 1: Task Boundaries and Action Tracking</a>
<a class="toc-link toc-h3" href="#concept-what-is-a-task">Concept: What is a "task"?</a>
<a class="toc-link toc-h3" href="#step-3-wrap-task-execution-with-agenttask">Step 3: Wrap task execution with agent.task()</a>
<a class="toc-link toc-h3" href="#alternative-manual-task-lifecycle-with-agentstart_task">Alternative: Manual task lifecycle with agent.start_task()</a>
<a class="toc-link toc-h3" href="#step-4-plumb-the-task-handle-through-the-call-stack">Step 4: Plumb the task handle through the call stack</a>
<a class="toc-link toc-h3" href="#step-5-track-key-functions-with-agenttrack">Step 5: Track key functions with @agent.track()</a>
<a class="toc-link toc-h3" href="#what-you-see-on-the-dashboard-after-layer-1">What you see on the dashboard after Layer 1</a>
<a class="toc-link" href="#layer-2a-llm-call-tracking">Layer 2a: LLM Call Tracking</a>
<a class="toc-link toc-h3" href="#step-6-find-all-llm-call-sites">Step 6: Find all LLM call sites</a>
<a class="toc-link toc-h3" href="#step-7-determine-token-extraction-for-each-site">Step 7: Determine token extraction for each site</a>
<a class="toc-link toc-h3" href="#step-8-add-taskllm_call-at-each-site">Step 8: Add task.llm_call() at each site</a>
<a class="toc-link toc-h3" href="#cost-estimation-helper">Cost estimation helper</a>
<a class="toc-link toc-h3" href="#optional-prompt-and-response-previews">Optional: Prompt and response previews</a>
<a class="toc-link toc-h3" href="#llm-call-metadata-unlocking-advanced-analytics">LLM call metadata: Unlocking advanced analytics</a>
<a class="toc-link toc-h3" href="#what-you-see-on-the-dashboard-after-layer-2a">What you see on the dashboard after Layer 2a</a>
<a class="toc-link" href="#layer-2b-tool-execution-tracking">Layer 2b: Tool Execution Tracking</a>
<a class="toc-link toc-h3" href="#the-problem">The problem</a>
<a class="toc-link toc-h3" href="#step-9-wrap-tool-dispatch-with-agenttrack_context">Step 9: Wrap tool dispatch with agent.track_context()</a>
<a class="toc-link toc-h3" href="#separated-lifecycle-pattern">Separated lifecycle pattern</a>
<a class="toc-link toc-h3" href="#standardized-tool-payloads-with-tool_payload">Standardized tool payloads with tool_payload()</a>
<a class="toc-link toc-h3" href="#what-you-see-on-the-dashboard-after-layer-2b">What you see on the dashboard after Layer 2b</a>
<a class="toc-link" href="#layer-2c-rich-events">Layer 2c: Rich Events</a>
<a class="toc-link toc-h3" href="#plans-and-plan-steps">Plans and plan steps</a>
<a class="toc-link toc-h3" href="#escalations">Escalations</a>
<a class="toc-link toc-h3" href="#approvals">Approvals</a>
<a class="toc-link toc-h3" href="#issues">Issues</a>
<a class="toc-link toc-h3" href="#log-forwarding-with-hiveboardloghandler">Log forwarding with HiveBoardLogHandler</a>
<a class="toc-link toc-h3" href="#retries">Retries</a>
<a class="toc-link toc-h3" href="#todos">TODOs</a>
<a class="toc-link toc-h3" href="#queue-snapshots">Queue snapshots</a>
<a class="toc-link toc-h3" href="#scheduled-work">Scheduled work</a>
<a class="toc-link" href="#layer-3-advanced-observability-patterns">Layer 3: Advanced Observability Patterns</a>
<a class="toc-link toc-h3" href="#learning-and-self-correction-events">Learning and self-correction events</a>
<a class="toc-link toc-h3" href="#context-compaction-tracking">Context compaction tracking</a>
<a class="toc-link toc-h3" href="#rich-error-context">Rich error context</a>
<a class="toc-link toc-h3" href="#runtime-lifecycle-events">Runtime lifecycle events</a>
<a class="toc-link toc-h3" href="#session-and-memory-operations">Session and memory operations</a>
<a class="toc-link toc-h3" href="#configuration-snapshots">Configuration snapshots</a>
<a class="toc-link" href="#layer-4-client-side-detection-patterns">Layer 4: Client-Side Detection Patterns</a>
<a class="toc-link toc-h3" href="#loop-and-cycle-detection">Loop and cycle detection</a>
<a class="toc-link toc-h3" href="#prompt-composition-breakdown">Prompt composition breakdown</a>
<a class="toc-link toc-h3" href="#state-mutation-tracking">State mutation tracking</a>
<a class="toc-link" href="#putting-it-all-together-complete-turn-instrumentation">Putting It All Together: Complete Turn Instrumentation</a>
<a class="toc-link" href="#architecture-patterns">Architecture Patterns</a>
<a class="toc-link toc-h3" href="#single-agent-system">Single-agent system</a>
<a class="toc-link toc-h3" href="#multi-agent-system">Multi-agent system</a>
<a class="toc-link toc-h3" href="#api-driven-agents-fastapi-flask">API-driven agents (FastAPI, Flask)</a>
<a class="toc-link toc-h3" href="#framework-callbacks-langchain-crewai-autogen">Framework callbacks (LangChain, CrewAI, AutoGen)</a>
<a class="toc-link" href="#incremental-adoption-strategy">Incremental Adoption Strategy</a>
<a class="toc-link toc-h3" href="#tier-1-highest-value-lowest-effort">Tier 1 (highest value, lowest effort)</a>
<a class="toc-link toc-h3" href="#tier-2-unlocks-debugging">Tier 2 (unlocks debugging)</a>
<a class="toc-link toc-h3" href="#tier-3-unlocks-operations">Tier 3 (unlocks operations)</a>
<a class="toc-link toc-h3" href="#tier-4-polish">Tier 4 (polish)</a>
<a class="toc-link toc-h3" href="#tier-5-advanced-analytics">Tier 5 (advanced analytics)</a>
<a class="toc-link" href="#common-mistakes">Common Mistakes</a>
<a class="toc-link" href="#shutdown">Shutdown</a>
<a class="toc-link" href="#quick-reference-all-sdk-methods">Quick Reference: All SDK Methods</a>
<a class="toc-link toc-h3" href="#module-level">Module-level</a>
<a class="toc-link toc-h3" href="#agent-level-hbagent-returns-this">Agent-level (hb.agent() returns this)</a>
<a class="toc-link toc-h3" href="#task-level-agenttask-yields-this">Task-level (agent.task() yields this)</a>
<a class="toc-link toc-h3" href="#context-manager-agenttrack_context-yields-this">Context manager (agent.track_context() yields this)</a>
<a class="toc-link toc-h3" href="#contrib-hiveloopcontrib">Contrib (hiveloop.contrib)</a>
<a class="toc-link toc-h3" href="#custom-event-kind-values-layer-3-4">Custom event kind values (Layer 3 & 4)</a>
</nav>

</div>

<!-- SCRIPTS -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-yaml.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-toml.min.js"></script>
<script>
// Re-highlight after load
Prism.highlightAll();

// Mobile menu toggle
document.getElementById('mobileMenuToggle')?.addEventListener('click', () => {
    document.getElementById('docsSidebar').classList.toggle('open');
});

// Close sidebar on content click (mobile)
document.querySelector('.docs-main')?.addEventListener('click', () => {
    document.getElementById('docsSidebar').classList.remove('open');
});

// Active TOC tracking
(function() {
    const tocLinks = document.querySelectorAll('.toc-link');
    if (!tocLinks.length) return;
    
    const headings = [];
    tocLinks.forEach(link => {
        const id = link.getAttribute('href')?.slice(1);
        const el = id && document.getElementById(id);
        if (el) headings.push({ el, link });
    });
    
    function updateActive() {
        let current = headings[0];
        for (const h of headings) {
            if (h.el.getBoundingClientRect().top <= 100) current = h;
        }
        tocLinks.forEach(l => l.classList.remove('active'));
        if (current) current.link.classList.add('active');
    }
    
    window.addEventListener('scroll', updateActive, { passive: true });
    updateActive();
})();

// Smooth scrolling for anchor links
document.querySelectorAll('a[href^="#"]').forEach(a => {
    a.addEventListener('click', e => {
        const target = document.querySelector(a.getAttribute('href'));
        if (target) {
            e.preventDefault();
            target.scrollIntoView({ behavior: 'smooth', block: 'start' });
            history.pushState(null, '', a.getAttribute('href'));
        }
    });
});
</script>
</body>
</html>
